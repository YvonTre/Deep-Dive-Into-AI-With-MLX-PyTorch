{"text": "Q: What is QuantizedLinear in mlx?\nA: mlx.nn.layers.quantized.QuantizedLinear Applies an affine transformation to the input using a quantized weight matrix.\n\nIt is the quantized equivalent of :class:`mlx.nn.Linear`. For now its\nparameters are frozen and will not be included in any gradient computation\nbut this will probably change in the future.\n\nQuantizedLinear also provides two useful classmethods to convert linear\nlayers to QuantizedLinear layers.\n\n- :meth:`from_linear` returns a QuantizedLinear layer that applies the same\n  linear transformation up to the quantization error.\n- :meth:`quantize_module` swaps all the linear layers of the passed module\n  with QuantizedLinear ones.\n\nArgs:\n    input_dims (int): The dimensionality of the input features\n    output_dims (int): The dimensionality of the output features\n    bias (bool, optional): If set to ``False`` then the layer will not use\n        a bias. (default: True).\n    group_size (int, optional): The group size to use for the quantized\n        weight. See :func:`~mlx.core.quantize`. (default: 64)\n    bits (int, optional): The bit width to use for the quantized weight.\n        See :func:`~mlx.core.quantize`. (default: 4)"}
{"text": "Q: What is ALiBi in mlx?\nA: mlx.nn.layers.ALiBi Base class for building neural networks with MLX.\n\nAll the layers provided in :mod:`mlx.nn.layers` subclass this class and\nyour models should do the same.\n\nA ``Module`` can contain other ``Module`` instances or :class:`mlx.core.array`\ninstances in arbitrary nesting of python lists or dicts. The ``Module``\nthen allows recursively extracting all the :class:`mlx.core.array` instances\nusing :meth:`mlx.nn.Module.parameters`.\n\nIn addition, the ``Module`` has the concept of trainable and non trainable\nparameters (called \"frozen\"). When using :func:`mlx.nn.value_and_grad`\nthe gradients are returned only with respect to the trainable parameters.\nAll arrays in a module are trainable unless they are added in the \"frozen\"\nset by calling :meth:`freeze`.\n\n.. code-block:: python\n\n    import mlx.core as mx\n    import mlx.nn as nn\n\n    class MyMLP(nn.Module):\n        def __init__(self, in_dims: int, out_dims: int, hidden_dims: int = 16):\n            super().__init__()\n\n            self.in_proj = nn.Linear(in_dims, hidden_dims)\n            self.out_proj = nn.Linear(hidden_dims, out_dims)\n\n        def __call__(self, x):\n            x = self.in_proj(x)\n            x = mx.maximum(x, 0)\n            return self.out_proj(x)\n\n    model = MyMLP(2, 1)\n\n    # All the model parameters are created but since MLX is lazy by\n    # default, they are not evaluated yet. Calling `mx.eval` actually\n    # allocates memory and initializes the parameters.\n    mx.eval(model.parameters())\n\n    # Setting a parameter to a new value is as simply as accessing that\n    # parameter and assigning a new array to it.\n    model.in_proj.weight = model.in_proj.weight * 2\n    mx.eval(model.parameters())"}
{"text": "Q: What is selu in mlx?\nA: mlx.nn.selu Applies the Scaled Exponential Linear Unit.\n\n.. math::\n    \\text{selu}(x) = \\begin{cases}\n    \\lambda x & \\text{if } x > 0 \\\\\n    \\lambda \\alpha (\\exp(x) - 1) & \\text{if } x \\leq 0\n    \\end{cases}\n\nwhere :math:`\\lambda = 1.0507` and :math:`\\alpha = 1.67326`.\n\nSee also :func:`elu`."}
{"text": "Q: What is relu6 in mlx?\nA: mlx.nn.layers.activations.relu6 Applies the Rectified Linear Unit 6.\n\nApplies :math:`\\min(\\max(x, 0), 6)` element wise."}
{"text": "Q: What is GroupNorm in mlx?\nA: mlx.nn.layers.GroupNorm Applies Group Normalization [1] to the inputs.\n\nComputes the same normalization as layer norm, namely\n\n.. math::\n\n    y = \\frac{x - E[x]}{\\sqrt{Var[x]} + \\epsilon} \\gamma + \\beta,\n\nwhere :math:`\\gamma` and :math:`\\beta` are learned per feature dimension\nparameters initialized at 1 and 0 respectively. However, the mean and\nvariance are computed over the spatial dimensions and each group of\nfeatures. In particular, the input is split into num_groups across the\nfeature dimension.\n\nThe feature dimension is assumed to be the last dimension and the dimensions\nthat precede it (except the first) are considered the spatial dimensions.\n\n[1]: https://arxiv.org/abs/1803.08494\n\nArgs:\n    num_groups (int): Number of groups to separate the features into\n    dims (int): The feature dimensions of the input to normalize over\n    eps (float): A small additive constant for numerical stability\n    affine (bool): If True learn an affine transform to apply after the\n        normalization.\n    pytorch_compatible (bool): If True perform the group normalization in\n        the same order/grouping as PyTorch."}
{"text": "Q: What is Embedding in mlx?\nA: mlx.nn.Embedding Implements a simple lookup table that maps each input integer to a\nhigh-dimensional vector.\n\nTypically used to embed discrete tokens for processing by neural networks.\n\nArgs:\n    num_embeddings (int): How many possible discrete tokens can we embed.\n                          Usually called the vocabulary size.\n    dims (int): The dimensionality of the embeddings."}
{"text": "Q: What is TransformerDecoderLayer in mlx?\nA: mlx.nn.layers.transformer.TransformerDecoderLayer Base class for building neural networks with MLX.\n\nAll the layers provided in :mod:`mlx.nn.layers` subclass this class and\nyour models should do the same.\n\nA ``Module`` can contain other ``Module`` instances or :class:`mlx.core.array`\ninstances in arbitrary nesting of python lists or dicts. The ``Module``\nthen allows recursively extracting all the :class:`mlx.core.array` instances\nusing :meth:`mlx.nn.Module.parameters`.\n\nIn addition, the ``Module`` has the concept of trainable and non trainable\nparameters (called \"frozen\"). When using :func:`mlx.nn.value_and_grad`\nthe gradients are returned only with respect to the trainable parameters.\nAll arrays in a module are trainable unless they are added in the \"frozen\"\nset by calling :meth:`freeze`.\n\n.. code-block:: python\n\n    import mlx.core as mx\n    import mlx.nn as nn\n\n    class MyMLP(nn.Module):\n        def __init__(self, in_dims: int, out_dims: int, hidden_dims: int = 16):\n            super().__init__()\n\n            self.in_proj = nn.Linear(in_dims, hidden_dims)\n            self.out_proj = nn.Linear(hidden_dims, out_dims)\n\n        def __call__(self, x):\n            x = self.in_proj(x)\n            x = mx.maximum(x, 0)\n            return self.out_proj(x)\n\n    model = MyMLP(2, 1)\n\n    # All the model parameters are created but since MLX is lazy by\n    # default, they are not evaluated yet. Calling `mx.eval` actually\n    # allocates memory and initializes the parameters.\n    mx.eval(model.parameters())\n\n    # Setting a parameter to a new value is as simply as accessing that\n    # parameter and assigning a new array to it.\n    model.in_proj.weight = model.in_proj.weight * 2\n    mx.eval(model.parameters())"}
{"text": "Q: What is CELU in mlx?\nA: mlx.nn.CELU Applies the Continuously Differentiable Exponential Linear Unit.\n    Applies :math:`\\max(0, x) + \\min(0, \\alpha * (\\exp(x / \\alpha) - 1))`\n    element wise.\n\nSee :func:`celu`, for the functional equivalent.\n\nArgs:\n    alpha: the :math:`\\alpha` value for the CELU formulation. Default: 1.0"}
{"text": "Q: What is elu in mlx?\nA: mlx.nn.layers.elu Applies the Exponential Linear Unit.\n\nSimply ``mx.where(x > 0, x, alpha * (mx.exp(x) - 1))``."}
{"text": "Q: What is log_sigmoid in mlx?\nA: mlx.nn.log_sigmoid Applies the Log Sigmoid function.\n\nApplies :math:`\\log(\\sigma(x)) = -\\log(1 + e^{-x})` element wise."}
