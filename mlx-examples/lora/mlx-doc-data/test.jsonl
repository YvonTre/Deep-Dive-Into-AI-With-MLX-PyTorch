{"text": "Q: What is ELU in mlx?\nA: mlx.nn.layers.activations.ELU Applies the Exponential Linear Unit.\n    Simply ``mx.where(x > 0, x, alpha * (mx.exp(x) - 1))``.\n\nSee :func:`elu`, for the functional equivalent.\n\nArgs:\n    alpha: the :math:`\\alpha` value for the ELU formulation. Default: 1.0"}
{"text": "Q: What is AdaDelta in mlx?\nA: mlx.optimizers.AdaDelta Implementation of the AdaDelta optimizer with learning rate[1].\n\nOur AdaDelta implementation follows the original paper. In detail,\n\n[1]: Zeiler, M.D., 2012. ADADELTA: an adaptive learning rate method. arXiv preprint arXiv:1212.5701.\n\n.. math::\n\n    v_{t+1} &= \\rho v_t + (1 - \\rho) g_t^2 \\\\\n    \\Delta w_{t+1} &= \\frac{\\sqrt{u_t + \\epsilon}}{\\sqrt{v_{t+1} + \\epsilon}} g_t \\\\\n    u_{t+1} &= \\rho u_t + (1 - \\rho) \\Delta w_{t+1}^2 \\\\\n    w_{t+1} &= w_t - \\lambda \\Delta w_{t+1}\n\nArgs:\n    learning_rate (float): The learning rate :math:`\\lambda`.\n    rho (float, optional): The coefficient :math:`\\rho` used for computing a\n        running average of squared gradients. Default: ``0.9``\n    eps (float, optional): The term :math:`\\epsilon` added to the denominator to improve\n      numerical stability. Ddefault: `1e-8`"}
{"text": "Q: What is relu in mlx?\nA: mlx.nn.layers.relu Applies the Rectified Linear Unit.\n\nSimply ``mx.maximum(x, 0)``."}
{"text": "Q: What is ELU in mlx?\nA: mlx.nn.layers.ELU Applies the Exponential Linear Unit.\n    Simply ``mx.where(x > 0, x, alpha * (mx.exp(x) - 1))``.\n\nSee :func:`elu`, for the functional equivalent.\n\nArgs:\n    alpha: the :math:`\\alpha` value for the ELU formulation. Default: 1.0"}
{"text": "Q: What is log_sigmoid in mlx?\nA: mlx.nn.layers.log_sigmoid Applies the Log Sigmoid function.\n\nApplies :math:`\\log(\\sigma(x)) = -\\log(1 + e^{-x})` element wise."}
{"text": "Q: What is DeviceType in mlx?\nA: mlx.core.DeviceType Members:\n\ncpu\n\ngpu"}
{"text": "Q: What is RoPE in mlx?\nA: mlx.nn.layers.positional_encoding.RoPE Implements the rotary positional encoding [1].\n\nThe traditional implementation rotates consecutive pairs of elements in the\nfeature dimension while the default implementation rotates pairs with\nstride half the feature dimensions for efficiency.\n\n[1]: https://arxiv.org/abs/2104.09864\n\nArgs:\n    dims (int): The feature dimensions to be rotated. If the input feature\n        is larger than dims then the rest is left unchanged.\n    traditional (bool, optional): If set to True choose the traditional\n        implementation which is slightly less efficient. Default: ``False``\n    base (float, optional): The base used to compute angular frequency for\n        each dimension in the positional encodings. Default: ``10000``"}
{"text": "Q: What is find_namespace_packages in mlx?\nA: mlx.extension.find_namespace_packages Return a list of all Python items (packages or modules, depending on\nthe finder implementation) found within directory 'where'.\n\n'where' is the root directory which will be searched.\nIt should be supplied as a \"cross-platform\" (i.e. URL-style) path;\nit will be converted to the appropriate local path syntax.\n\n'exclude' is a sequence of names to exclude; '*' can be used\nas a wildcard in the names.\nWhen finding packages, 'foo.*' will exclude all subpackages of 'foo'\n(but not 'foo' itself).\n\n'include' is a sequence of names to include.\nIf it's specified, only the named items will be included.\nIf it's not specified, all found items will be included.\n'include' can contain shell style wildcard patterns just like\n'exclude'."}
{"text": "Q: What is Softplus in mlx?\nA: mlx.nn.Softplus Applies the Softplus function.\n\nApplies :math:`\\log(1 + \\exp(x))` element wise."}
