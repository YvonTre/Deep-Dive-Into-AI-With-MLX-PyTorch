{"text": "Q: What is prelu in mlx?\nA: mlx.nn.layers.activations.prelu Applies the element-wise function:\n\n.. math::\n    \\text{PReLU}(x) = \\max(0,x) + a * \\min(0,x)\n\nHere :math:`a` is an array."}
{"text": "Q: What is log_sigmoid in mlx?\nA: mlx.nn.layers.activations.log_sigmoid Applies the Log Sigmoid function.\n\nApplies :math:`\\log(\\sigma(x)) = -\\log(1 + e^{-x})` element wise."}
{"text": "Q: What is LayerNorm in mlx?\nA: mlx.nn.layers.transformer.LayerNorm Applies layer normalization [1] on the inputs.\n\nComputes\n\n.. math::\n\n    y = \\frac{x - E[x]}{\\sqrt{Var[x]} + \\epsilon} \\gamma + \\beta,\n\nwhere :math:`\\gamma` and :math:`\\beta` are learned per feature dimension\nparameters initialized at 1 and 0 respectively.\n\n[1]: https://arxiv.org/abs/1607.06450\n\nArgs:\n    dims (int): The feature dimension of the input to normalize over\n    eps (float): A small additive constant for numerical stability\n    affine (bool): If True learn an affine transform to apply after the\n        normalization"}
{"text": "Q: What is tree_map in mlx?\nA: mlx.optimizers.tree_map Applies ``fn`` to the leaves of the python tree ``tree`` and\nreturns a new collection with the results.\n\nIf ``rest`` is provided, every item is assumed to be a superset of ``tree``\nand the corresponding leaves are provided as extra positional arguments to\n``fn``. In that respect, :meth:`tree_map` is closer to :func:`itertools.starmap`\nthan to :func:`map`.\n\nThe keyword argument ``is_leaf`` decides what constitutes a leaf from\n``tree`` similar to :func:`tree_flatten`.\n\n.. code-block:: python\n\n    import mlx.nn as nn\n    from mlx.utils import tree_map\n\n    model = nn.Linear(10, 10)\n    print(model.parameters().keys())\n    # dict_keys(['weight', 'bias'])\n\n    # square the parameters\n    model.update(tree_map(lambda x: x*x, model.parameters()))\n\nArgs:\n    fn (Callable): The function that processes the leaves of the tree\n    tree (Any): The main python tree that will be iterated upon\n    rest (Tuple[Any]): Extra trees to be iterated together with tree\n    is_leaf (Optional[Callable]): An optional callable that returns True if\n        the passed object is considered a leaf or False otherwise.\n\nReturns:\n    A python tree with the new values returned by ``fn``."}
{"text": "Q: What is SGD in mlx?\nA: mlx.optimizers.SGD Stochastic gradient descent optimizer.\n\nUpdates a parameter :math:`w` with a gradient :math:`g` as follows\n\n.. math::\n\n    v_{t+1} &= \\mu v_t + (1 - \\tau) g_t \\\\\n    w_{t+1} &= w_t - \\lambda v_{t+1}\n\nArgs:\n    learning_rate (float): The learning rate :math:`\\lambda`.\n    momentum (float, optional): The momentum strength :math:`\\mu`. Default: ``0``\n    weight_decay (float, optional): The weight decay (L2 penalty). Default: ``0``\n    dampening (float, optional): Dampening for momentum :math:`\\tau`. Default: ``0``\n    nesterov (bool, optional): Enables Nesterov momentum. Default: ``False``"}
{"text": "Q: What is silu in mlx?\nA: mlx.nn.silu Applies the Sigmoid Linear Unit. Also known as Swish.\n\nApplies :math:`x \\sigma(x)` element wise, where :math:`\\sigma(\\cdot)` is\nthe logistic sigmoid."}
{"text": "Q: What is setup in mlx?\nA: mlx.extension.setup The gateway to the Distutils: do everything your setup script needs\nto do, in a highly flexible and user-driven way.  Briefly: create a\nDistribution instance; find and parse config files; parse the command\nline; run each Distutils command found there, customized by the options\nsupplied to 'setup()' (as keyword arguments), in config files, and on\nthe command line.\n\nThe Distribution instance might be an instance of a class supplied via\nthe 'distclass' keyword argument to 'setup'; if no such class is\nsupplied, then the Distribution class (in dist.py) is instantiated.\nAll other arguments to 'setup' (except for 'cmdclass') are used to set\nattributes of the Distribution instance.\n\nThe 'cmdclass' argument, if supplied, is a dictionary mapping command\nnames to command classes.  Each command encountered on the command line\nwill be turned into a command class, which is in turn instantiated; any\nclass found in 'cmdclass' is used in place of the default, which is\n(for command 'foo_bar') class 'foo_bar' in module\n'distutils.command.foo_bar'.  The command class must provide a\n'user_options' attribute which is a list of option specifiers for\n'distutils.fancy_getopt'.  Any command-line options between the current\nand the next command are used to set attributes of the current command\nobject.\n\nWhen the entire command-line has been successfully parsed, calls the\n'run()' method on each command object in turn.  This method will be\ndriven entirely by the Distribution object (which each command object\nhas a reference to, thanks to its constructor), and the\ncommand-specific options that became attributes of each command\nobject."}
{"text": "Q: What is GroupNorm in mlx?\nA: mlx.nn.layers.normalization.GroupNorm Applies Group Normalization [1] to the inputs.\n\nComputes the same normalization as layer norm, namely\n\n.. math::\n\n    y = \\frac{x - E[x]}{\\sqrt{Var[x]} + \\epsilon} \\gamma + \\beta,\n\nwhere :math:`\\gamma` and :math:`\\beta` are learned per feature dimension\nparameters initialized at 1 and 0 respectively. However, the mean and\nvariance are computed over the spatial dimensions and each group of\nfeatures. In particular, the input is split into num_groups across the\nfeature dimension.\n\nThe feature dimension is assumed to be the last dimension and the dimensions\nthat precede it (except the first) are considered the spatial dimensions.\n\n[1]: https://arxiv.org/abs/1803.08494\n\nArgs:\n    num_groups (int): Number of groups to separate the features into\n    dims (int): The feature dimensions of the input to normalize over\n    eps (float): A small additive constant for numerical stability\n    affine (bool): If True learn an affine transform to apply after the\n        normalization.\n    pytorch_compatible (bool): If True perform the group normalization in\n        the same order/grouping as PyTorch."}
{"text": "Q: What is SinusoidalPositionalEncoding in mlx?\nA: mlx.nn.layers.SinusoidalPositionalEncoding Implements sinusoidal positional encoding similar to [1].\n\n[1]: https://arxiv.org/abs/1706.03762\n\nArgs:\n    dims (int): The dimensionality of the resulting positional embeddings.\n    min_freq (float): The minimum frequency expected (default: 0.0001)\n    max_freq (float): The maximum frequency expected (default: 1)\n    scale (float): Scale the embeddings by that number (default: sqrt(dims//2))\n    cos_first (bool): If set to True embed using ``[cos(x); sin(x)]``\n        instead of the other way around (default: False)\n    full_turns (bool): If set to True multiply the frequencies\n        with ``2 pi`` (default: False)"}
{"text": "Q: What is binary_cross_entropy in mlx?\nA: mlx.nn.losses.binary_cross_entropy Computes the binary cross entropy loss.\n\nArgs:\n    logits (array): The unnormalized (pre-sigmoid) predicted logits.\n    targets (array): The binary target values in {0, 1}.\n    reduction (str, optional): Specifies the reduction to apply to the output:\n      ``'none'`` | ``'mean'`` | ``'sum'``. Default: ``'none'``.\n\nReturns:\n    array: The computed binary cross entropy loss.\nExamples:\n    >>> import mlx.core as mx\n    >>> import mlx.nn as nn\n    >>> inputs = mx.array([0.105361, 0.223144, 1.20397, 0.916291])\n    >>> targets = mx.array([0, 0, 1, 1])\n    >>> loss = nn.losses.binary_cross_entropy(inputs, targets, \"mean\")\n    >>> loss\n    array([0.612192], dtype=float32)"}
{"text": "Q: What is step in mlx?\nA: mlx.nn.layers.step Applies the Step Activation Function.\n\nThis function implements a binary step activation, where the output is set\nto 1 if the input is greater than a specified threshold, and 0 otherwise.\n\n.. math::\n    \\text{step}(x) = \\begin{cases}\n    0 & \\text{if } x < \\text{threshold} \\\\\n    1 & \\text{if } x \\geq \\text{threshold}\n    \\end{cases}\n\nArgs:\n    threshold: The value to threshold at."}
{"text": "Q: What is prelu in mlx?\nA: mlx.nn.prelu Applies the element-wise function:\n\n.. math::\n    \\text{PReLU}(x) = \\max(0,x) + a * \\min(0,x)\n\nHere :math:`a` is an array."}
{"text": "Q: What is LayerNorm in mlx?\nA: mlx.nn.layers.LayerNorm Applies layer normalization [1] on the inputs.\n\nComputes\n\n.. math::\n\n    y = \\frac{x - E[x]}{\\sqrt{Var[x]} + \\epsilon} \\gamma + \\beta,\n\nwhere :math:`\\gamma` and :math:`\\beta` are learned per feature dimension\nparameters initialized at 1 and 0 respectively.\n\n[1]: https://arxiv.org/abs/1607.06450\n\nArgs:\n    dims (int): The feature dimension of the input to normalize over\n    eps (float): A small additive constant for numerical stability\n    affine (bool): If True learn an affine transform to apply after the\n        normalization"}
{"text": "Q: What is MultiHeadAttention in mlx?\nA: mlx.nn.MultiHeadAttention Implements the scaled dot product attention with multiple heads.\n\nGiven inputs for queries, keys and values the ``MultiHeadAttention`` produces\nnew values by aggregating information from the input values according to\nthe similarities of the input queries and keys.\n\nAll inputs as well as the output are linearly projected without biases.\n\nMultiHeadAttention also expects an additive attention mask that should be\nbroadcastable with (batch, num_heads, # queries, # keys). The mask should\nhave ``-inf`` or very negative numbers to the positions that should *not* be\nattended to.\n\nArgs:\n    dims (int): The model dimensions. If no other dims are provided then\n        dims is used for queries, keys, values and the output.\n    num_heads (int): How many attention heads to use\n    query_input_dims (int, optional): The input dimensions of the queries (default: dims).\n    key_input_dims (int, optional): The input dimensions of the keys (default: dims).\n    value_input_dims (int, optional): The input dimensions of the values (default: key_input_dims).\n    value_dims (int, optional): The dimensions of the values after the projection (default: dims).\n    value_output_dims (int, optional): The dimensions the new values will be projected to (default: dims)."}
{"text": "Q: What is MultiHeadAttention in mlx?\nA: mlx.nn.layers.transformer.MultiHeadAttention Implements the scaled dot product attention with multiple heads.\n\nGiven inputs for queries, keys and values the ``MultiHeadAttention`` produces\nnew values by aggregating information from the input values according to\nthe similarities of the input queries and keys.\n\nAll inputs as well as the output are linearly projected without biases.\n\nMultiHeadAttention also expects an additive attention mask that should be\nbroadcastable with (batch, num_heads, # queries, # keys). The mask should\nhave ``-inf`` or very negative numbers to the positions that should *not* be\nattended to.\n\nArgs:\n    dims (int): The model dimensions. If no other dims are provided then\n        dims is used for queries, keys, values and the output.\n    num_heads (int): How many attention heads to use\n    query_input_dims (int, optional): The input dimensions of the queries (default: dims).\n    key_input_dims (int, optional): The input dimensions of the keys (default: dims).\n    value_input_dims (int, optional): The input dimensions of the values (default: key_input_dims).\n    value_dims (int, optional): The dimensions of the values after the projection (default: dims).\n    value_output_dims (int, optional): The dimensions the new values will be projected to (default: dims)."}
{"text": "Q: What is build_ext in mlx?\nA: mlx.extension.build_ext Setuptools internal actions are organized using a *command design pattern*.\nThis means that each action (or group of closely related actions) executed during\nthe build should be implemented as a ``Command`` subclass.\n\nThese commands are abstractions and do not necessarily correspond to a command that\ncan (or should) be executed via a terminal, in a CLI fashion (although historically\nthey would).\n\nWhen creating a new command from scratch, custom defined classes **SHOULD** inherit\nfrom ``setuptools.Command`` and implement a few mandatory methods.\nBetween these mandatory methods, are listed:\n\n.. method:: initialize_options(self)\n\n    Set or (reset) all options/attributes/caches used by the command\n    to their default values. Note that these values may be overwritten during\n    the build.\n\n.. method:: finalize_options(self)\n\n    Set final values for all options/attributes used by the command.\n    Most of the time, each option/attribute/cache should only be set if it does not\n    have any value yet (e.g. ``if self.attr is None: self.attr = val``).\n\n.. method:: run(self)\n\n    Execute the actions intended by the command.\n    (Side effects **SHOULD** only take place when ``run`` is executed,\n    for example, creating new files or writing to the terminal output).\n\nA useful analogy for command classes is to think of them as subroutines with local\nvariables called \"options\".  The options are \"declared\" in ``initialize_options()``\nand \"defined\" (given their final values, aka \"finalized\") in ``finalize_options()``,\nboth of which must be defined by every command class. The \"body\" of the subroutine,\n(where it does all the work) is the ``run()`` method.\nBetween ``initialize_options()`` and ``finalize_options()``, ``setuptools`` may set\nthe values for options/attributes based on user's input (or circumstance),\nwhich means that the implementation should be careful to not overwrite values in\n``finalize_options`` unless necessary.\n\nPlease note that other commands (or other parts of setuptools) may also overwrite\nthe values of the c"}
{"text": "Q: What is ELU in mlx?\nA: mlx.nn.ELU Applies the Exponential Linear Unit.\n    Simply ``mx.where(x > 0, x, alpha * (mx.exp(x) - 1))``.\n\nSee :func:`elu`, for the functional equivalent.\n\nArgs:\n    alpha: the :math:`\\alpha` value for the ELU formulation. Default: 1.0"}
{"text": "Q: What is leaky_relu in mlx?\nA: mlx.nn.layers.activations.leaky_relu Applies the Leaky Rectified Linear Unit.\n\nSimply ``mx.maximum(negative_slope * x, x)``."}
{"text": "Q: What is Transformer in mlx?\nA: mlx.nn.layers.transformer.Transformer Base class for building neural networks with MLX.\n\nAll the layers provided in :mod:`mlx.nn.layers` subclass this class and\nyour models should do the same.\n\nA ``Module`` can contain other ``Module`` instances or :class:`mlx.core.array`\ninstances in arbitrary nesting of python lists or dicts. The ``Module``\nthen allows recursively extracting all the :class:`mlx.core.array` instances\nusing :meth:`mlx.nn.Module.parameters`.\n\nIn addition, the ``Module`` has the concept of trainable and non trainable\nparameters (called \"frozen\"). When using :func:`mlx.nn.value_and_grad`\nthe gradients are returned only with respect to the trainable parameters.\nAll arrays in a module are trainable unless they are added in the \"frozen\"\nset by calling :meth:`freeze`.\n\n.. code-block:: python\n\n    import mlx.core as mx\n    import mlx.nn as nn\n\n    class MyMLP(nn.Module):\n        def __init__(self, in_dims: int, out_dims: int, hidden_dims: int = 16):\n            super().__init__()\n\n            self.in_proj = nn.Linear(in_dims, hidden_dims)\n            self.out_proj = nn.Linear(hidden_dims, out_dims)\n\n        def __call__(self, x):\n            x = self.in_proj(x)\n            x = mx.maximum(x, 0)\n            return self.out_proj(x)\n\n    model = MyMLP(2, 1)\n\n    # All the model parameters are created but since MLX is lazy by\n    # default, they are not evaluated yet. Calling `mx.eval` actually\n    # allocates memory and initializes the parameters.\n    mx.eval(model.parameters())\n\n    # Setting a parameter to a new value is as simply as accessing that\n    # parameter and assigning a new array to it.\n    model.in_proj.weight = model.in_proj.weight * 2\n    mx.eval(model.parameters())"}
{"text": "Q: What is SiLU in mlx?\nA: mlx.nn.SiLU Applies the Sigmoid Linear Unit. Also known as Swish.\n\nApplies :math:`x \\sigma(x)` element wise, where :math:`\\sigma(\\cdot)` is\nthe logistic sigmoid."}
{"text": "Q: What is SinusoidalPositionalEncoding in mlx?\nA: mlx.nn.SinusoidalPositionalEncoding Implements sinusoidal positional encoding similar to [1].\n\n[1]: https://arxiv.org/abs/1706.03762\n\nArgs:\n    dims (int): The dimensionality of the resulting positional embeddings.\n    min_freq (float): The minimum frequency expected (default: 0.0001)\n    max_freq (float): The maximum frequency expected (default: 1)\n    scale (float): Scale the embeddings by that number (default: sqrt(dims//2))\n    cos_first (bool): If set to True embed using ``[cos(x); sin(x)]``\n        instead of the other way around (default: False)\n    full_turns (bool): If set to True multiply the frequencies\n        with ``2 pi`` (default: False)"}
{"text": "Q: What is tanh in mlx?\nA: mlx.nn.layers.activations.tanh Applies the hyperbolic tangent function.\n\nSimply ``mx.tanh(x)``."}
{"text": "Q: What is Conv2d in mlx?\nA: mlx.nn.layers.Conv2d Applies a 2-dimensional convolution over the multi-channel input image.\n\nThe channels are expected to be last i.e. the input shape should be ``NHWC`` where:\n    - ``N`` is the batch dimension\n    - ``H`` is the input image height\n    - ``W`` is the input image width\n    - ``C`` is the number of input channels\n\nArgs:\n    in_channels (int): The number of input channels.\n    out_channels (int): The number of output channels.\n    kernel_size (int or tuple): The size of the convolution filters.\n    stride (int or tuple, optional): The size of the stride when\n        applying the filter. Default: 1.\n    padding (int or tuple, optional): How many positions to 0-pad\n        the input with. Default: 0.\n    bias (bool, optional): If ``True`` add a learnable bias to the\n        output. Default: ``True``"}
{"text": "Q: What is tree_map in mlx?\nA: mlx.nn.layers.quantized.tree_map Applies ``fn`` to the leaves of the python tree ``tree`` and\nreturns a new collection with the results.\n\nIf ``rest`` is provided, every item is assumed to be a superset of ``tree``\nand the corresponding leaves are provided as extra positional arguments to\n``fn``. In that respect, :meth:`tree_map` is closer to :func:`itertools.starmap`\nthan to :func:`map`.\n\nThe keyword argument ``is_leaf`` decides what constitutes a leaf from\n``tree`` similar to :func:`tree_flatten`.\n\n.. code-block:: python\n\n    import mlx.nn as nn\n    from mlx.utils import tree_map\n\n    model = nn.Linear(10, 10)\n    print(model.parameters().keys())\n    # dict_keys(['weight', 'bias'])\n\n    # square the parameters\n    model.update(tree_map(lambda x: x*x, model.parameters()))\n\nArgs:\n    fn (Callable): The function that processes the leaves of the tree\n    tree (Any): The main python tree that will be iterated upon\n    rest (Tuple[Any]): Extra trees to be iterated together with tree\n    is_leaf (Optional[Callable]): An optional callable that returns True if\n        the passed object is considered a leaf or False otherwise.\n\nReturns:\n    A python tree with the new values returned by ``fn``."}
{"text": "Q: What is step in mlx?\nA: mlx.nn.step Applies the Step Activation Function.\n\nThis function implements a binary step activation, where the output is set\nto 1 if the input is greater than a specified threshold, and 0 otherwise.\n\n.. math::\n    \\text{step}(x) = \\begin{cases}\n    0 & \\text{if } x < \\text{threshold} \\\\\n    1 & \\text{if } x \\geq \\text{threshold}\n    \\end{cases}\n\nArgs:\n    threshold: The value to threshold at."}
{"text": "Q: What is QuantizedLinear in mlx?\nA: mlx.nn.layers.QuantizedLinear Applies an affine transformation to the input using a quantized weight matrix.\n\nIt is the quantized equivalent of :class:`mlx.nn.Linear`. For now its\nparameters are frozen and will not be included in any gradient computation\nbut this will probably change in the future.\n\nQuantizedLinear also provides two useful classmethods to convert linear\nlayers to QuantizedLinear layers.\n\n- :meth:`from_linear` returns a QuantizedLinear layer that applies the same\n  linear transformation up to the quantization error.\n- :meth:`quantize_module` swaps all the linear layers of the passed module\n  with QuantizedLinear ones.\n\nArgs:\n    input_dims (int): The dimensionality of the input features\n    output_dims (int): The dimensionality of the output features\n    bias (bool, optional): If set to ``False`` then the layer will not use\n        a bias. (default: True).\n    group_size (int, optional): The group size to use for the quantized\n        weight. See :func:`~mlx.core.quantize`. (default: 64)\n    bits (int, optional): The bit width to use for the quantized weight.\n        See :func:`~mlx.core.quantize`. (default: 4)"}
{"text": "Q: What is CMakeBuild in mlx?\nA: mlx.extension.CMakeBuild Setuptools internal actions are organized using a *command design pattern*.\nThis means that each action (or group of closely related actions) executed during\nthe build should be implemented as a ``Command`` subclass.\n\nThese commands are abstractions and do not necessarily correspond to a command that\ncan (or should) be executed via a terminal, in a CLI fashion (although historically\nthey would).\n\nWhen creating a new command from scratch, custom defined classes **SHOULD** inherit\nfrom ``setuptools.Command`` and implement a few mandatory methods.\nBetween these mandatory methods, are listed:\n\n.. method:: initialize_options(self)\n\n    Set or (reset) all options/attributes/caches used by the command\n    to their default values. Note that these values may be overwritten during\n    the build.\n\n.. method:: finalize_options(self)\n\n    Set final values for all options/attributes used by the command.\n    Most of the time, each option/attribute/cache should only be set if it does not\n    have any value yet (e.g. ``if self.attr is None: self.attr = val``).\n\n.. method:: run(self)\n\n    Execute the actions intended by the command.\n    (Side effects **SHOULD** only take place when ``run`` is executed,\n    for example, creating new files or writing to the terminal output).\n\nA useful analogy for command classes is to think of them as subroutines with local\nvariables called \"options\".  The options are \"declared\" in ``initialize_options()``\nand \"defined\" (given their final values, aka \"finalized\") in ``finalize_options()``,\nboth of which must be defined by every command class. The \"body\" of the subroutine,\n(where it does all the work) is the ``run()`` method.\nBetween ``initialize_options()`` and ``finalize_options()``, ``setuptools`` may set\nthe values for options/attributes based on user's input (or circumstance),\nwhich means that the implementation should be careful to not overwrite values in\n``finalize_options`` unless necessary.\n\nPlease note that other commands (or other parts of setuptools) may also overwrite\nthe values of the c"}
{"text": "Q: What is gelu in mlx?\nA: mlx.nn.layers.gelu Applies the Gaussian Error Linear Units function.\n\n.. math::\n    \\\\textrm{GELU}(x) = x * \\Phi(x)\n\nwhere :math:`\\Phi(x)` is the Gaussian CDF.\n\nSee also :func:`gelu_approx` and :func:`gelu_fast_approx` for faster\napproximations."}
{"text": "Q: What is celu in mlx?\nA: mlx.nn.layers.celu Applies the Continuously Differentiable Exponential Linear Unit.\n\nApplies :math:`\\max(0, x) + \\min(0, \\alpha * (\\exp(x / \\alpha) - 1))`\nelement wise."}
{"text": "Q: What is ReLU in mlx?\nA: mlx.nn.ReLU Applies the Rectified Linear Unit.\n\nSimply ``mx.maximum(x, 0)``."}
{"text": "Q: What is LogSigmoid in mlx?\nA: mlx.nn.LogSigmoid Applies the Log Sigmoid function.\n\nApplies :math:`\\log(\\sigma(x)) = -\\log(1 + e^{-x})` element wise."}
{"text": "Q: What is CELU in mlx?\nA: mlx.nn.layers.activations.CELU Applies the Continuously Differentiable Exponential Linear Unit.\n    Applies :math:`\\max(0, x) + \\min(0, \\alpha * (\\exp(x / \\alpha) - 1))`\n    element wise.\n\nSee :func:`celu`, for the functional equivalent.\n\nArgs:\n    alpha: the :math:`\\alpha` value for the CELU formulation. Default: 1.0"}
{"text": "Q: What is SELU in mlx?\nA: mlx.nn.layers.SELU Applies the Scaled Exponential Linear Unit.\n\n.. math::\n    \\text{selu}(x) = \\begin{cases}\n    \\lambda x & \\text{if } x > 0 \\\\\n    \\lambda \\alpha (\\exp(x) - 1) & \\text{if } x \\leq 0\n    \\end{cases}\n\nwhere :math:`\\lambda = 1.0507` and :math:`\\alpha = 1.67326`.\n\nSee also :func:`elu`."}
{"text": "Q: What is MultiHeadAttention in mlx?\nA: mlx.nn.layers.MultiHeadAttention Implements the scaled dot product attention with multiple heads.\n\nGiven inputs for queries, keys and values the ``MultiHeadAttention`` produces\nnew values by aggregating information from the input values according to\nthe similarities of the input queries and keys.\n\nAll inputs as well as the output are linearly projected without biases.\n\nMultiHeadAttention also expects an additive attention mask that should be\nbroadcastable with (batch, num_heads, # queries, # keys). The mask should\nhave ``-inf`` or very negative numbers to the positions that should *not* be\nattended to.\n\nArgs:\n    dims (int): The model dimensions. If no other dims are provided then\n        dims is used for queries, keys, values and the output.\n    num_heads (int): How many attention heads to use\n    query_input_dims (int, optional): The input dimensions of the queries (default: dims).\n    key_input_dims (int, optional): The input dimensions of the keys (default: dims).\n    value_input_dims (int, optional): The input dimensions of the values (default: key_input_dims).\n    value_dims (int, optional): The dimensions of the values after the projection (default: dims).\n    value_output_dims (int, optional): The dimensions the new values will be projected to (default: dims)."}
{"text": "Q: What is silu in mlx?\nA: mlx.nn.layers.activations.silu Applies the Sigmoid Linear Unit. Also known as Swish.\n\nApplies :math:`x \\sigma(x)` element wise, where :math:`\\sigma(\\cdot)` is\nthe logistic sigmoid."}
{"text": "Q: What is Module in mlx?\nA: mlx.nn.layers.Module Base class for building neural networks with MLX.\n\nAll the layers provided in :mod:`mlx.nn.layers` subclass this class and\nyour models should do the same.\n\nA ``Module`` can contain other ``Module`` instances or :class:`mlx.core.array`\ninstances in arbitrary nesting of python lists or dicts. The ``Module``\nthen allows recursively extracting all the :class:`mlx.core.array` instances\nusing :meth:`mlx.nn.Module.parameters`.\n\nIn addition, the ``Module`` has the concept of trainable and non trainable\nparameters (called \"frozen\"). When using :func:`mlx.nn.value_and_grad`\nthe gradients are returned only with respect to the trainable parameters.\nAll arrays in a module are trainable unless they are added in the \"frozen\"\nset by calling :meth:`freeze`.\n\n.. code-block:: python\n\n    import mlx.core as mx\n    import mlx.nn as nn\n\n    class MyMLP(nn.Module):\n        def __init__(self, in_dims: int, out_dims: int, hidden_dims: int = 16):\n            super().__init__()\n\n            self.in_proj = nn.Linear(in_dims, hidden_dims)\n            self.out_proj = nn.Linear(hidden_dims, out_dims)\n\n        def __call__(self, x):\n            x = self.in_proj(x)\n            x = mx.maximum(x, 0)\n            return self.out_proj(x)\n\n    model = MyMLP(2, 1)\n\n    # All the model parameters are created but since MLX is lazy by\n    # default, they are not evaluated yet. Calling `mx.eval` actually\n    # allocates memory and initializes the parameters.\n    mx.eval(model.parameters())\n\n    # Setting a parameter to a new value is as simply as accessing that\n    # parameter and assigning a new array to it.\n    model.in_proj.weight = model.in_proj.weight * 2\n    mx.eval(model.parameters())"}
{"text": "Q: What is LeakyReLU in mlx?\nA: mlx.nn.LeakyReLU Applies the Leaky Rectified Linear Unit.\n\nSimply ``mx.maximum(negative_slope * x, x)``.\n\nArgs:\n    negative_slope: Controls the angle of the negative slope. Default: 1e-2."}
{"text": "Q: What is Mish in mlx?\nA: mlx.nn.layers.activations.Mish Applies the Mish function, element-wise.\nMish: A Self Regularized Non-Monotonic Neural Activation Function.\n\nReference: https://arxiv.org/abs/1908.08681\n\n.. math::\n    \\text{Mish}(x) = x * \\text{Tanh}(\\text{Softplus}(x))"}
{"text": "Q: What is step in mlx?\nA: mlx.nn.layers.activations.step Applies the Step Activation Function.\n\nThis function implements a binary step activation, where the output is set\nto 1 if the input is greater than a specified threshold, and 0 otherwise.\n\n.. math::\n    \\text{step}(x) = \\begin{cases}\n    0 & \\text{if } x < \\text{threshold} \\\\\n    1 & \\text{if } x \\geq \\text{threshold}\n    \\end{cases}\n\nArgs:\n    threshold: The value to threshold at."}
{"text": "Q: What is Conv2d in mlx?\nA: mlx.nn.layers.convolution.Conv2d Applies a 2-dimensional convolution over the multi-channel input image.\n\nThe channels are expected to be last i.e. the input shape should be ``NHWC`` where:\n    - ``N`` is the batch dimension\n    - ``H`` is the input image height\n    - ``W`` is the input image width\n    - ``C`` is the number of input channels\n\nArgs:\n    in_channels (int): The number of input channels.\n    out_channels (int): The number of output channels.\n    kernel_size (int or tuple): The size of the convolution filters.\n    stride (int or tuple, optional): The size of the stride when\n        applying the filter. Default: 1.\n    padding (int or tuple, optional): How many positions to 0-pad\n        the input with. Default: 0.\n    bias (bool, optional): If ``True`` add a learnable bias to the\n        output. Default: ``True``"}
{"text": "Q: What is tree_flatten in mlx?\nA: mlx.nn.layers.base.tree_flatten Flattens a python tree to a list of key, value tuples.\n\nThe keys are using the dot notation to define trees of arbitrary depth and\ncomplexity.\n\n.. code-block:: python\n\n    from mlx.utils import tree_flatten\n\n    print(tree_flatten([[[0]]]))\n    # [(\"0.0.0\", 0)]\n\n    print(tree_flatten([[[0]]], \".hello\"))\n    # [(\"hello.0.0.0\", 0)]\n\n.. note::\n   Dictionaries should have keys that are valid python identifiers.\n\nArgs:\n    tree (Any): The python tree to be flattened.\n    prefix (str): A prefix to use for the keys. The first character is\n        always discarded.\n    is_leaf (Callable): An optional callable that returns True if the\n        passed object is considered a leaf or False otherwise.\n\nReturns:\n    List[Tuple[str, Any]]: The flat representation of the python tree."}
{"text": "Q: What is value_and_grad in mlx?\nA: mlx.nn.utils.value_and_grad Transform the passed function ``fn`` to a function that computes the\ngradients of ``fn`` wrt the model's trainable parameters and also its\nvalue.\n\nArgs:\n    model (mlx.nn.Module): The model whose trainable parameters to compute\n                           gradients for\n    fn (Callable): The scalar function to compute gradients for\n\nReturns:\n    A callable that returns the value of ``fn`` and the gradients wrt the\n    trainable parameters of ``model``"}
{"text": "Q: What is Module in mlx?\nA: mlx.nn.layers.containers.Module Base class for building neural networks with MLX.\n\nAll the layers provided in :mod:`mlx.nn.layers` subclass this class and\nyour models should do the same.\n\nA ``Module`` can contain other ``Module`` instances or :class:`mlx.core.array`\ninstances in arbitrary nesting of python lists or dicts. The ``Module``\nthen allows recursively extracting all the :class:`mlx.core.array` instances\nusing :meth:`mlx.nn.Module.parameters`.\n\nIn addition, the ``Module`` has the concept of trainable and non trainable\nparameters (called \"frozen\"). When using :func:`mlx.nn.value_and_grad`\nthe gradients are returned only with respect to the trainable parameters.\nAll arrays in a module are trainable unless they are added in the \"frozen\"\nset by calling :meth:`freeze`.\n\n.. code-block:: python\n\n    import mlx.core as mx\n    import mlx.nn as nn\n\n    class MyMLP(nn.Module):\n        def __init__(self, in_dims: int, out_dims: int, hidden_dims: int = 16):\n            super().__init__()\n\n            self.in_proj = nn.Linear(in_dims, hidden_dims)\n            self.out_proj = nn.Linear(hidden_dims, out_dims)\n\n        def __call__(self, x):\n            x = self.in_proj(x)\n            x = mx.maximum(x, 0)\n            return self.out_proj(x)\n\n    model = MyMLP(2, 1)\n\n    # All the model parameters are created but since MLX is lazy by\n    # default, they are not evaluated yet. Calling `mx.eval` actually\n    # allocates memory and initializes the parameters.\n    mx.eval(model.parameters())\n\n    # Setting a parameter to a new value is as simply as accessing that\n    # parameter and assigning a new array to it.\n    model.in_proj.weight = model.in_proj.weight * 2\n    mx.eval(model.parameters())"}
{"text": "Q: What is LogSigmoid in mlx?\nA: mlx.nn.layers.activations.LogSigmoid Applies the Log Sigmoid function.\n\nApplies :math:`\\log(\\sigma(x)) = -\\log(1 + e^{-x})` element wise."}
{"text": "Q: What is RoPE in mlx?\nA: mlx.nn.RoPE Implements the rotary positional encoding [1].\n\nThe traditional implementation rotates consecutive pairs of elements in the\nfeature dimension while the default implementation rotates pairs with\nstride half the feature dimensions for efficiency.\n\n[1]: https://arxiv.org/abs/2104.09864\n\nArgs:\n    dims (int): The feature dimensions to be rotated. If the input feature\n        is larger than dims then the rest is left unchanged.\n    traditional (bool, optional): If set to True choose the traditional\n        implementation which is slightly less efficient. Default: ``False``\n    base (float, optional): The base used to compute angular frequency for\n        each dimension in the positional encodings. Default: ``10000``"}
{"text": "Q: What is mish in mlx?\nA: mlx.nn.layers.activations.mish Applies the Mish function, element-wise.\nMish: A Self Regularized Non-Monotonic Neural Activation Function.\n\nReference: https://arxiv.org/abs/1908.08681\n\n.. math::\n    \\text{Mish}(x) = x * \\text{Tanh}(\\text{Softplus}(x))"}
{"text": "Q: What is Embedding in mlx?\nA: mlx.nn.layers.embedding.Embedding Implements a simple lookup table that maps each input integer to a\nhigh-dimensional vector.\n\nTypically used to embed discrete tokens for processing by neural networks.\n\nArgs:\n    num_embeddings (int): How many possible discrete tokens can we embed.\n                          Usually called the vocabulary size.\n    dims (int): The dimensionality of the embeddings."}
{"text": "Q: What is Any in mlx?\nA: mlx.nn.layers.base.Any Special type indicating an unconstrained type.\n\n- Any is compatible with every type.\n- Any assumed to have all methods.\n- All values assumed to be instances of Any.\n\nNote that all the above statements are true from the point of view of\nstatic type checkers. At runtime, Any should not be used with instance\nchecks."}
{"text": "Q: What is FixedRepr in mlx?\nA: mlx._reprlib_fix.FixedRepr Only route python array instances to repr_array."}
{"text": "Q: What is Lion in mlx?\nA: mlx.optimizers.Lion Implementation of the Lion optimizer [1].\n\nSince updates are computed through the sign operation, they tend to\nhave larger norm than for other optimizers such as SGD and Adam.\nWe recommend a learning rate that is 3-10x smaller than AdamW and a\nweight decay 3-10x larger than AdamW to maintain the strength\n(lr * wd). Our Lion implementation follows the original paper. In\ndetail,\n\n[1]: Chen, X. Symbolic Discovery of Optimization Algorithms. arXiv\npreprint arXiv:2302.06675.\n\n.. math::\n\n    c_{t + 1} &= \\beta_1 m_t + (1 - \\beta_1) g_t\n    m_{t + 1} &= \\beta_2 m_t + (1 - \\beta_2) g_t\n    w_{t + 1} &= w_t - \\eta (\\text{sign}(c_t) + \\lambda w_t)\n\nArgs:\n    learning_rate (float): The learning rate :math:`\\eta`.\n    betas (Tuple[float, float], optional): The coefficients\n      :math:`(\\beta_1, \\beta_2)` used for computing the gradient\n      momentum and update direction. Default: ``(0.9, 0.99)``\n    weight_decay (float, optional): The weight decay :math:`\\lambda`. Default: ``0.0``"}
{"text": "Q: What is Conv1d in mlx?\nA: mlx.nn.layers.Conv1d Applies a 1-dimensional convolution over the multi-channel input sequence.\n\nThe channels are expected to be last i.e. the input shape should be ``NLC`` where:\n    - ``N`` is the batch dimension\n    - ``L`` is the sequence length\n    - ``C`` is the number of input channels\n\nArgs:\n    in_channels (int): The number of input channels\n    out_channels (int): The number of output channels\n    kernel_size (int): The size of the convolution filters\n    stride (int, optional): The stride when applying the filter.\n        Default: 1.\n    padding (int, optional): How many positions to 0-pad the input with.\n        Default: 0.\n    bias (bool, optional): If ``True`` add a learnable bias to the output.\n        Default: ``True``"}
{"text": "Q: What is LeakyReLU in mlx?\nA: mlx.nn.layers.activations.LeakyReLU Applies the Leaky Rectified Linear Unit.\n\nSimply ``mx.maximum(negative_slope * x, x)``.\n\nArgs:\n    negative_slope: Controls the angle of the negative slope. Default: 1e-2."}
{"text": "Q: What is SELU in mlx?\nA: mlx.nn.SELU Applies the Scaled Exponential Linear Unit.\n\n.. math::\n    \\text{selu}(x) = \\begin{cases}\n    \\lambda x & \\text{if } x > 0 \\\\\n    \\lambda \\alpha (\\exp(x) - 1) & \\text{if } x \\leq 0\n    \\end{cases}\n\nwhere :math:`\\lambda = 1.0507` and :math:`\\alpha = 1.67326`.\n\nSee also :func:`elu`."}
{"text": "Q: What is Linear in mlx?\nA: mlx.nn.layers.linear.Linear Applies an affine transformation to the input.\n\nConcretely:\n\n.. math::\n\n    y = W^\\top x + b\n\nwhere :math:`W` has shape ``[output_dims, input_dims]``.\n\nArgs:\n    input_dims (int): The dimensionality of the input features\n    output_dims (int): The dimensionality of the output features\n    bias (bool, optional): If set to ``False`` then the layer will\n      not use a bias. Default ``True``."}
{"text": "Q: What is leaky_relu in mlx?\nA: mlx.nn.layers.leaky_relu Applies the Leaky Rectified Linear Unit.\n\nSimply ``mx.maximum(negative_slope * x, x)``."}
{"text": "Q: What is tree_unflatten in mlx?\nA: mlx.utils.tree_unflatten Recreate a python tree from its flat representation.\n\n.. code-block:: python\n\n    from mlx.utils import tree_unflatten\n\n    d = tree_unflatten([(\"hello.world\", 42)])\n    print(d)\n    # {\"hello\": {\"world\": 42}}\n\nArgs:\n    tree (List[Tuple[str, Any]]): The flat representation of a python tree.\n                                  For instance as returned by :meth:`tree_flatten`.\n\nReturns:\n    A python tree."}
{"text": "Q: What is Sequential in mlx?\nA: mlx.nn.layers.containers.Sequential A layer that calls the passed callables in order.\n\nWe can pass either modules or plain callables to the Sequential module. If\nour functions have learnable parameters they should be implemented as\n``nn.Module`` instances.\n\nArgs:\n    modules (tuple of Callables): The modules to call in order"}
{"text": "Q: What is Path in mlx?\nA: mlx.extension.Path PurePath subclass that can make system calls.\n\nPath represents a filesystem path but unlike PurePath, also offers\nmethods to do system calls on path objects. Depending on your system,\ninstantiating a Path will return either a PosixPath or a WindowsPath\nobject. You can also instantiate a PosixPath or WindowsPath directly,\nbut cannot instantiate a WindowsPath on a POSIX system or vice versa."}
{"text": "Q: What is Linear in mlx?\nA: mlx.nn.Linear Applies an affine transformation to the input.\n\nConcretely:\n\n.. math::\n\n    y = W^\\top x + b\n\nwhere :math:`W` has shape ``[output_dims, input_dims]``.\n\nArgs:\n    input_dims (int): The dimensionality of the input features\n    output_dims (int): The dimensionality of the output features\n    bias (bool, optional): If set to ``False`` then the layer will\n      not use a bias. Default ``True``."}
{"text": "Q: What is TransformerEncoderLayer in mlx?\nA: mlx.nn.layers.TransformerEncoderLayer Base class for building neural networks with MLX.\n\nAll the layers provided in :mod:`mlx.nn.layers` subclass this class and\nyour models should do the same.\n\nA ``Module`` can contain other ``Module`` instances or :class:`mlx.core.array`\ninstances in arbitrary nesting of python lists or dicts. The ``Module``\nthen allows recursively extracting all the :class:`mlx.core.array` instances\nusing :meth:`mlx.nn.Module.parameters`.\n\nIn addition, the ``Module`` has the concept of trainable and non trainable\nparameters (called \"frozen\"). When using :func:`mlx.nn.value_and_grad`\nthe gradients are returned only with respect to the trainable parameters.\nAll arrays in a module are trainable unless they are added in the \"frozen\"\nset by calling :meth:`freeze`.\n\n.. code-block:: python\n\n    import mlx.core as mx\n    import mlx.nn as nn\n\n    class MyMLP(nn.Module):\n        def __init__(self, in_dims: int, out_dims: int, hidden_dims: int = 16):\n            super().__init__()\n\n            self.in_proj = nn.Linear(in_dims, hidden_dims)\n            self.out_proj = nn.Linear(hidden_dims, out_dims)\n\n        def __call__(self, x):\n            x = self.in_proj(x)\n            x = mx.maximum(x, 0)\n            return self.out_proj(x)\n\n    model = MyMLP(2, 1)\n\n    # All the model parameters are created but since MLX is lazy by\n    # default, they are not evaluated yet. Calling `mx.eval` actually\n    # allocates memory and initializes the parameters.\n    mx.eval(model.parameters())\n\n    # Setting a parameter to a new value is as simply as accessing that\n    # parameter and assigning a new array to it.\n    model.in_proj.weight = model.in_proj.weight * 2\n    mx.eval(model.parameters())"}
{"text": "Q: What is Mish in mlx?\nA: mlx.nn.layers.Mish Applies the Mish function, element-wise.\nMish: A Self Regularized Non-Monotonic Neural Activation Function.\n\nReference: https://arxiv.org/abs/1908.08681\n\n.. math::\n    \\text{Mish}(x) = x * \\text{Tanh}(\\text{Softplus}(x))"}
{"text": "Q: What is l1_loss in mlx?\nA: mlx.nn.losses.l1_loss Computes the L1 loss.\n\nArgs:\n    predictions (array): The predicted values.\n    targets (array): The target values.\n    reduction (str, optional): Specifies the reduction to apply to the output:\n      ``'none'`` | ``'mean'`` | ``'sum'``. Default: ``'mean'``.\n\nReturns:\n    array: The computed L1 loss."}
{"text": "Q: What is Softplus in mlx?\nA: mlx.nn.layers.activations.Softplus Applies the Softplus function.\n\nApplies :math:`\\log(1 + \\exp(x))` element wise."}
{"text": "Q: What is LeakyReLU in mlx?\nA: mlx.nn.layers.LeakyReLU Applies the Leaky Rectified Linear Unit.\n\nSimply ``mx.maximum(negative_slope * x, x)``.\n\nArgs:\n    negative_slope: Controls the angle of the negative slope. Default: 1e-2."}
{"text": "Q: What is AdamW in mlx?\nA: mlx.optimizers.AdamW Implementation of the AdamW optimizer [1].\n\nFollowing the above convention, in contrast with [1], we do not use bias\ncorrection in the first and second moments for AdamW. We update the weights\nwith a weight_decay (:math:`\\lambda`) value:\n\n[1]: Loshchilov, I. and Hutter, F., 2019. Decoupled weight decay\nregularization. ICLR 2019.\n\n.. math::\n\n    m_{t+1} &= \\beta_1 m_t + (1 - \\beta_1) g_t \\\\\n    v_{t+1} &= \\beta_2 v_t + (1 - \\beta_2) g_t^2 \\\\\n    w_{t+1} &= w_t - \\alpha (\\frac{m_{t+1}}{\\sqrt{v_{t+1} + \\epsilon}} + \\lambda w_t)\n\nArgs:\n    learning_rate (float): The learning rate :math:`\\alpha`.\n    betas (Tuple[float, float], optional): The coefficients\n      :math:`(\\beta_1, \\beta_2)` used for computing running averages of the\n      gradient and its square. Default: ``(0.9, 0.999)``\n    eps (float, optional): The term :math:`\\epsilon` added to the\n      denominator to improve numerical stability. Default: ``1e-8``\n    weight_decay (float, optional): The weight decay :math:`\\lambda`.\n      Default: ``0``."}
{"text": "Q: What is Linear in mlx?\nA: mlx.nn.layers.quantized.Linear Applies an affine transformation to the input.\n\nConcretely:\n\n.. math::\n\n    y = W^\\top x + b\n\nwhere :math:`W` has shape ``[output_dims, input_dims]``.\n\nArgs:\n    input_dims (int): The dimensionality of the input features\n    output_dims (int): The dimensionality of the output features\n    bias (bool, optional): If set to ``False`` then the layer will\n      not use a bias. Default ``True``."}
{"text": "Q: What is Dropout in mlx?\nA: mlx.nn.layers.dropout.Dropout Randomly zero a portion of the elements during training.\n\nThe remaining elements are multiplied with :math:`\frac{1}{1-p}` where\n:math:`p` is the probability of zeroing an element. This is done so the\nexpected value of a given element will remain the same.\n\nArgs:\n    p (float): The probability to zero an element"}
{"text": "Q: What is relu6 in mlx?\nA: mlx.nn.layers.relu6 Applies the Rectified Linear Unit 6.\n\nApplies :math:`\\min(\\max(x, 0), 6)` element wise."}
{"text": "Q: What is LayerNorm in mlx?\nA: mlx.nn.LayerNorm Applies layer normalization [1] on the inputs.\n\nComputes\n\n.. math::\n\n    y = \\frac{x - E[x]}{\\sqrt{Var[x]} + \\epsilon} \\gamma + \\beta,\n\nwhere :math:`\\gamma` and :math:`\\beta` are learned per feature dimension\nparameters initialized at 1 and 0 respectively.\n\n[1]: https://arxiv.org/abs/1607.06450\n\nArgs:\n    dims (int): The feature dimension of the input to normalize over\n    eps (float): A small additive constant for numerical stability\n    affine (bool): If True learn an affine transform to apply after the\n        normalization"}
{"text": "Q: What is ReLU in mlx?\nA: mlx.nn.layers.ReLU Applies the Rectified Linear Unit.\n\nSimply ``mx.maximum(x, 0)``."}
{"text": "Q: What is tree_map in mlx?\nA: mlx.utils.tree_map Applies ``fn`` to the leaves of the python tree ``tree`` and\nreturns a new collection with the results.\n\nIf ``rest`` is provided, every item is assumed to be a superset of ``tree``\nand the corresponding leaves are provided as extra positional arguments to\n``fn``. In that respect, :meth:`tree_map` is closer to :func:`itertools.starmap`\nthan to :func:`map`.\n\nThe keyword argument ``is_leaf`` decides what constitutes a leaf from\n``tree`` similar to :func:`tree_flatten`.\n\n.. code-block:: python\n\n    import mlx.nn as nn\n    from mlx.utils import tree_map\n\n    model = nn.Linear(10, 10)\n    print(model.parameters().keys())\n    # dict_keys(['weight', 'bias'])\n\n    # square the parameters\n    model.update(tree_map(lambda x: x*x, model.parameters()))\n\nArgs:\n    fn (Callable): The function that processes the leaves of the tree\n    tree (Any): The main python tree that will be iterated upon\n    rest (Tuple[Any]): Extra trees to be iterated together with tree\n    is_leaf (Optional[Callable]): An optional callable that returns True if\n        the passed object is considered a leaf or False otherwise.\n\nReturns:\n    A python tree with the new values returned by ``fn``."}
{"text": "Q: What is SinusoidalPositionalEncoding in mlx?\nA: mlx.nn.layers.positional_encoding.SinusoidalPositionalEncoding Implements sinusoidal positional encoding similar to [1].\n\n[1]: https://arxiv.org/abs/1706.03762\n\nArgs:\n    dims (int): The dimensionality of the resulting positional embeddings.\n    min_freq (float): The minimum frequency expected (default: 0.0001)\n    max_freq (float): The maximum frequency expected (default: 1)\n    scale (float): Scale the embeddings by that number (default: sqrt(dims//2))\n    cos_first (bool): If set to True embed using ``[cos(x); sin(x)]``\n        instead of the other way around (default: False)\n    full_turns (bool): If set to True multiply the frequencies\n        with ``2 pi`` (default: False)"}
{"text": "Q: What is cross_entropy in mlx?\nA: mlx.nn.losses.cross_entropy Computes the cross entropy loss.\n\nArgs:\n    logits (array): The unnormalized predicted logits.\n    targets (array): The target values, as class indices.\n    weights (array, optional): Weights for each target. Default: ``None``.\n    axis (int, optional): The axis over which to compute softmax. Default: ``-1``.\n    label_smoothing (float, optional): Label smoothing factor. Default: ``0``.\n    reduction (str, optional): Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``. Default: ``'none'``.\n\nReturns:\n    array: The computed cross entropy loss."}
{"text": "Q: What is celu in mlx?\nA: mlx.nn.celu Applies the Continuously Differentiable Exponential Linear Unit.\n\nApplies :math:`\\max(0, x) + \\min(0, \\alpha * (\\exp(x / \\alpha) - 1))`\nelement wise."}
{"text": "Q: What is RoPE in mlx?\nA: mlx.nn.layers.RoPE Implements the rotary positional encoding [1].\n\nThe traditional implementation rotates consecutive pairs of elements in the\nfeature dimension while the default implementation rotates pairs with\nstride half the feature dimensions for efficiency.\n\n[1]: https://arxiv.org/abs/2104.09864\n\nArgs:\n    dims (int): The feature dimensions to be rotated. If the input feature\n        is larger than dims then the rest is left unchanged.\n    traditional (bool, optional): If set to True choose the traditional\n        implementation which is slightly less efficient. Default: ``False``\n    base (float, optional): The base used to compute angular frequency for\n        each dimension in the positional encodings. Default: ``10000``"}
{"text": "Q: What is GroupNorm in mlx?\nA: mlx.nn.GroupNorm Applies Group Normalization [1] to the inputs.\n\nComputes the same normalization as layer norm, namely\n\n.. math::\n\n    y = \\frac{x - E[x]}{\\sqrt{Var[x]} + \\epsilon} \\gamma + \\beta,\n\nwhere :math:`\\gamma` and :math:`\\beta` are learned per feature dimension\nparameters initialized at 1 and 0 respectively. However, the mean and\nvariance are computed over the spatial dimensions and each group of\nfeatures. In particular, the input is split into num_groups across the\nfeature dimension.\n\nThe feature dimension is assumed to be the last dimension and the dimensions\nthat precede it (except the first) are considered the spatial dimensions.\n\n[1]: https://arxiv.org/abs/1803.08494\n\nArgs:\n    num_groups (int): Number of groups to separate the features into\n    dims (int): The feature dimensions of the input to normalize over\n    eps (float): A small additive constant for numerical stability\n    affine (bool): If True learn an affine transform to apply after the\n        normalization.\n    pytorch_compatible (bool): If True perform the group normalization in\n        the same order/grouping as PyTorch."}
{"text": "Q: What is Conv1d in mlx?\nA: mlx.nn.Conv1d Applies a 1-dimensional convolution over the multi-channel input sequence.\n\nThe channels are expected to be last i.e. the input shape should be ``NLC`` where:\n    - ``N`` is the batch dimension\n    - ``L`` is the sequence length\n    - ``C`` is the number of input channels\n\nArgs:\n    in_channels (int): The number of input channels\n    out_channels (int): The number of output channels\n    kernel_size (int): The size of the convolution filters\n    stride (int, optional): The stride when applying the filter.\n        Default: 1.\n    padding (int, optional): How many positions to 0-pad the input with.\n        Default: 0.\n    bias (bool, optional): If ``True`` add a learnable bias to the output.\n        Default: ``True``"}
{"text": "Q: What is ReLU6 in mlx?\nA: mlx.nn.layers.ReLU6 Applies the Rectified Linear Unit 6.\n\nApplies :math:`\\min(\\max(x, 0), 6)` element wise."}
{"text": "Q: What is value_and_grad in mlx?\nA: mlx.nn.value_and_grad Transform the passed function ``fn`` to a function that computes the\ngradients of ``fn`` wrt the model's trainable parameters and also its\nvalue.\n\nArgs:\n    model (mlx.nn.Module): The model whose trainable parameters to compute\n                           gradients for\n    fn (Callable): The scalar function to compute gradients for\n\nReturns:\n    A callable that returns the value of ``fn`` and the gradients wrt the\n    trainable parameters of ``model``"}
{"text": "Q: What is PReLU in mlx?\nA: mlx.nn.PReLU Base class for building neural networks with MLX.\n\nAll the layers provided in :mod:`mlx.nn.layers` subclass this class and\nyour models should do the same.\n\nA ``Module`` can contain other ``Module`` instances or :class:`mlx.core.array`\ninstances in arbitrary nesting of python lists or dicts. The ``Module``\nthen allows recursively extracting all the :class:`mlx.core.array` instances\nusing :meth:`mlx.nn.Module.parameters`.\n\nIn addition, the ``Module`` has the concept of trainable and non trainable\nparameters (called \"frozen\"). When using :func:`mlx.nn.value_and_grad`\nthe gradients are returned only with respect to the trainable parameters.\nAll arrays in a module are trainable unless they are added in the \"frozen\"\nset by calling :meth:`freeze`.\n\n.. code-block:: python\n\n    import mlx.core as mx\n    import mlx.nn as nn\n\n    class MyMLP(nn.Module):\n        def __init__(self, in_dims: int, out_dims: int, hidden_dims: int = 16):\n            super().__init__()\n\n            self.in_proj = nn.Linear(in_dims, hidden_dims)\n            self.out_proj = nn.Linear(hidden_dims, out_dims)\n\n        def __call__(self, x):\n            x = self.in_proj(x)\n            x = mx.maximum(x, 0)\n            return self.out_proj(x)\n\n    model = MyMLP(2, 1)\n\n    # All the model parameters are created but since MLX is lazy by\n    # default, they are not evaluated yet. Calling `mx.eval` actually\n    # allocates memory and initializes the parameters.\n    mx.eval(model.parameters())\n\n    # Setting a parameter to a new value is as simply as accessing that\n    # parameter and assigning a new array to it.\n    model.in_proj.weight = model.in_proj.weight * 2\n    mx.eval(model.parameters())"}
{"text": "Q: What is triplet_loss in mlx?\nA: mlx.nn.losses.triplet_loss Computes the triplet loss for a set of anchor, positive, and negative samples.\nMargin is represented with alpha in the math section.\n\n.. math::\n\n   L_{\\text{triplet}} = \\max\\left(\\|A - P\\|_p - \\|A - N\\|_p + \\alpha, 0\\right)\n\nArgs:\n    anchors (array): The anchor samples.\n    positives (array): The positive samples.\n    negatives (array): The negative samples.\n    axis (int, optional): The distribution axis. Default: ``-1``.\n    p (int, optional): The norm degree for pairwise distance. Default: ``2``.\n    margin (float, optional): Margin for the triplet loss. Defaults to ``1.0``.\n    eps (float, optional): Small positive constant to prevent numerical instability. Defaults to ``1e-6``.\n    reduction (str, optional): Specifies the reduction to apply to the output:\n      ``'none'`` | ``'mean'`` | ``'sum'``. Default: ``'none'``.\n\nReturns:\n    array: Computed triplet loss. If reduction is \"none\", returns a tensor of the same shape as input;\n              if reduction is \"mean\" or \"sum\", returns a scalar tensor."}
{"text": "Q: What is ALiBi in mlx?\nA: mlx.nn.layers.positional_encoding.ALiBi Base class for building neural networks with MLX.\n\nAll the layers provided in :mod:`mlx.nn.layers` subclass this class and\nyour models should do the same.\n\nA ``Module`` can contain other ``Module`` instances or :class:`mlx.core.array`\ninstances in arbitrary nesting of python lists or dicts. The ``Module``\nthen allows recursively extracting all the :class:`mlx.core.array` instances\nusing :meth:`mlx.nn.Module.parameters`.\n\nIn addition, the ``Module`` has the concept of trainable and non trainable\nparameters (called \"frozen\"). When using :func:`mlx.nn.value_and_grad`\nthe gradients are returned only with respect to the trainable parameters.\nAll arrays in a module are trainable unless they are added in the \"frozen\"\nset by calling :meth:`freeze`.\n\n.. code-block:: python\n\n    import mlx.core as mx\n    import mlx.nn as nn\n\n    class MyMLP(nn.Module):\n        def __init__(self, in_dims: int, out_dims: int, hidden_dims: int = 16):\n            super().__init__()\n\n            self.in_proj = nn.Linear(in_dims, hidden_dims)\n            self.out_proj = nn.Linear(hidden_dims, out_dims)\n\n        def __call__(self, x):\n            x = self.in_proj(x)\n            x = mx.maximum(x, 0)\n            return self.out_proj(x)\n\n    model = MyMLP(2, 1)\n\n    # All the model parameters are created but since MLX is lazy by\n    # default, they are not evaluated yet. Calling `mx.eval` actually\n    # allocates memory and initializes the parameters.\n    mx.eval(model.parameters())\n\n    # Setting a parameter to a new value is as simply as accessing that\n    # parameter and assigning a new array to it.\n    model.in_proj.weight = model.in_proj.weight * 2\n    mx.eval(model.parameters())"}
{"text": "Q: What is prelu in mlx?\nA: mlx.nn.layers.prelu Applies the element-wise function:\n\n.. math::\n    \\text{PReLU}(x) = \\max(0,x) + a * \\min(0,x)\n\nHere :math:`a` is an array."}
{"text": "Q: What is TransformerDecoder in mlx?\nA: mlx.nn.layers.transformer.TransformerDecoder Base class for building neural networks with MLX.\n\nAll the layers provided in :mod:`mlx.nn.layers` subclass this class and\nyour models should do the same.\n\nA ``Module`` can contain other ``Module`` instances or :class:`mlx.core.array`\ninstances in arbitrary nesting of python lists or dicts. The ``Module``\nthen allows recursively extracting all the :class:`mlx.core.array` instances\nusing :meth:`mlx.nn.Module.parameters`.\n\nIn addition, the ``Module`` has the concept of trainable and non trainable\nparameters (called \"frozen\"). When using :func:`mlx.nn.value_and_grad`\nthe gradients are returned only with respect to the trainable parameters.\nAll arrays in a module are trainable unless they are added in the \"frozen\"\nset by calling :meth:`freeze`.\n\n.. code-block:: python\n\n    import mlx.core as mx\n    import mlx.nn as nn\n\n    class MyMLP(nn.Module):\n        def __init__(self, in_dims: int, out_dims: int, hidden_dims: int = 16):\n            super().__init__()\n\n            self.in_proj = nn.Linear(in_dims, hidden_dims)\n            self.out_proj = nn.Linear(hidden_dims, out_dims)\n\n        def __call__(self, x):\n            x = self.in_proj(x)\n            x = mx.maximum(x, 0)\n            return self.out_proj(x)\n\n    model = MyMLP(2, 1)\n\n    # All the model parameters are created but since MLX is lazy by\n    # default, they are not evaluated yet. Calling `mx.eval` actually\n    # allocates memory and initializes the parameters.\n    mx.eval(model.parameters())\n\n    # Setting a parameter to a new value is as simply as accessing that\n    # parameter and assigning a new array to it.\n    model.in_proj.weight = model.in_proj.weight * 2\n    mx.eval(model.parameters())"}
{"text": "Q: What is kl_div_loss in mlx?\nA: mlx.nn.losses.kl_div_loss Computes the Kullback-Leibler divergence loss.\n\nComputes the following when ``reduction == 'none'``:\n\n.. code-block:: python\n\n    mx.exp(targets) * (targets - inputs).sum(axis)\n\nArgs:\n    inputs (array): Log probabilities for the predicted distribution.\n    targets (array): Log probabilities for the target distribution.\n    axis (int, optional): The distribution axis. Default: ``-1``.\n    reduction (str, optional): Specifies the reduction to apply to the output:\n      ``'none'`` | ``'mean'`` | ``'sum'``. Default: ``'none'``.\n\nReturns:\n    array: The computed Kullback-Leibler divergence loss."}
{"text": "Q: What is Module in mlx?\nA: mlx.nn.layers.embedding.Module Base class for building neural networks with MLX.\n\nAll the layers provided in :mod:`mlx.nn.layers` subclass this class and\nyour models should do the same.\n\nA ``Module`` can contain other ``Module`` instances or :class:`mlx.core.array`\ninstances in arbitrary nesting of python lists or dicts. The ``Module``\nthen allows recursively extracting all the :class:`mlx.core.array` instances\nusing :meth:`mlx.nn.Module.parameters`.\n\nIn addition, the ``Module`` has the concept of trainable and non trainable\nparameters (called \"frozen\"). When using :func:`mlx.nn.value_and_grad`\nthe gradients are returned only with respect to the trainable parameters.\nAll arrays in a module are trainable unless they are added in the \"frozen\"\nset by calling :meth:`freeze`.\n\n.. code-block:: python\n\n    import mlx.core as mx\n    import mlx.nn as nn\n\n    class MyMLP(nn.Module):\n        def __init__(self, in_dims: int, out_dims: int, hidden_dims: int = 16):\n            super().__init__()\n\n            self.in_proj = nn.Linear(in_dims, hidden_dims)\n            self.out_proj = nn.Linear(hidden_dims, out_dims)\n\n        def __call__(self, x):\n            x = self.in_proj(x)\n            x = mx.maximum(x, 0)\n            return self.out_proj(x)\n\n    model = MyMLP(2, 1)\n\n    # All the model parameters are created but since MLX is lazy by\n    # default, they are not evaluated yet. Calling `mx.eval` actually\n    # allocates memory and initializes the parameters.\n    mx.eval(model.parameters())\n\n    # Setting a parameter to a new value is as simply as accessing that\n    # parameter and assigning a new array to it.\n    model.in_proj.weight = model.in_proj.weight * 2\n    mx.eval(model.parameters())"}
{"text": "Q: What is Dropout in mlx?\nA: mlx.nn.Dropout Randomly zero a portion of the elements during training.\n\nThe remaining elements are multiplied with :math:`\frac{1}{1-p}` where\n:math:`p` is the probability of zeroing an element. This is done so the\nexpected value of a given element will remain the same.\n\nArgs:\n    p (float): The probability to zero an element"}
{"text": "Q: What is gelu_approx in mlx?\nA: mlx.nn.gelu_approx An approximation to Gaussian Error Linear Unit.\n\nSee :func:`gelu` for the exact computation.\n\nThis function approximates ``gelu`` with a maximum absolute error :math:`<\n0.0003` in the range :math:`[-6, 6]` using the following\n\n.. math::\n\n    x = x \\sigma\\left(1.60033 x \\left(1 + 0.0433603 x^2\\right)\\right)\n\nwhere :math:`\\sigma(\\cdot)` is the logistic sigmoid."}
{"text": "Q: What is Embedding in mlx?\nA: mlx.nn.layers.Embedding Implements a simple lookup table that maps each input integer to a\nhigh-dimensional vector.\n\nTypically used to embed discrete tokens for processing by neural networks.\n\nArgs:\n    num_embeddings (int): How many possible discrete tokens can we embed.\n                          Usually called the vocabulary size.\n    dims (int): The dimensionality of the embeddings."}
{"text": "Q: What is mse_loss in mlx?\nA: mlx.nn.losses.mse_loss Computes the mean squared error loss.\n\nArgs:\n    predictions (array): The predicted values.\n    targets (array): The target values.\n    reduction (str, optional): Specifies the reduction to apply to the output:\n      ``'none'`` | ``'mean'`` | ``'sum'``. Default: ``'mean'``.\n\nReturns:\n    array: The computed mean squared error loss."}
{"text": "Q: What is smooth_l1_loss in mlx?\nA: mlx.nn.losses.smooth_l1_loss Computes the smooth L1 loss.\n\nThe smooth L1 loss is a variant of the L1 loss which replaces the absolute\ndifference with a squared difference when the absolute difference is less\nthan ``beta``.\n\nThe formula for the smooth L1 Loss is:\n\n.. math::\n\n   l =\n      \\begin{cases}\n        0.5 (x - y)^2, & \\text{ if } & (x - y) < \\beta \\\\\n        |x - y| - 0.5 \\beta, &  & \\text{otherwise}\n      \\end{cases}\n\nArgs:\n    predictions (array): Predicted values.\n    targets (array): Ground truth values.\n    beta (float, optional): The threshold after which the loss changes\n      from the squared to the absolute difference. Default: ``1.0``.\n    reduction (str, optional): Specifies the reduction to apply to the output:\n      ``'none'`` | ``'mean'`` | ``'sum'``. Default: ``'mean'``.\n\nReturns:\n    array: The computed smooth L1 loss."}
{"text": "Q: What is tree_unflatten in mlx?\nA: mlx.nn.layers.base.tree_unflatten Recreate a python tree from its flat representation.\n\n.. code-block:: python\n\n    from mlx.utils import tree_unflatten\n\n    d = tree_unflatten([(\"hello.world\", 42)])\n    print(d)\n    # {\"hello\": {\"world\": 42}}\n\nArgs:\n    tree (List[Tuple[str, Any]]): The flat representation of a python tree.\n                                  For instance as returned by :meth:`tree_flatten`.\n\nReturns:\n    A python tree."}
{"text": "Q: What is Dropout in mlx?\nA: mlx.nn.layers.Dropout Randomly zero a portion of the elements during training.\n\nThe remaining elements are multiplied with :math:`\frac{1}{1-p}` where\n:math:`p` is the probability of zeroing an element. This is done so the\nexpected value of a given element will remain the same.\n\nArgs:\n    p (float): The probability to zero an element"}
{"text": "Q: What is Adamax in mlx?\nA: mlx.optimizers.Adamax Implementation of the Adamax optimizer. It is a variant of Adam based\non the infinity norm [1].\n\nOur Adam implementation follows the original paper and omits the bias\ncorrection in the first and second moment estimates. In detail,\n\n[1]: Kingma, D.P. and Ba, J., 2015. Adam: A method for stochastic\noptimization. ICLR 2015.\n\n.. math::\n\n    m_{t+1} &= \\beta_1 m_t + (1 - \\beta_1) g_t \\\\\n    v_{t+1} &= \\max(\\beta_2 v_t, |g_t|) \\\\\n    w_{t+1} &= w_t - \\lambda \\frac{m_{t+1}}{v_{t+1} + \\epsilon}\n\nArgs:\n    learning_rate (float): The learning rate :math:`\\lambda`.\n    betas (Tuple[float, float], optional): The coefficients\n      :math:`(\\beta_1, \\beta_2)` used for computing running averages of the\n      gradient and its square. Default: ``(0.9, 0.999)``\n    eps (float, optional): The term :math:`\\epsilon` added to the\n      denominator to improve numerical stability. Default: ``1e-8``"}
{"text": "Q: What is Adagrad in mlx?\nA: mlx.optimizers.Adagrad Implementation of the Adagrad optimizer [1].\n\nOur Adagrad implementation follows the original paper. In detail,\n\n[1]: Duchi, J., Hazan, E. and Singer, Y., 2011. Adaptive subgradient methods\nfor online learning and stochastic optimization. JMLR 2011.\n\n.. math::\n\n    v_{t+1} &= v_t + g_t^2 \\\\\n    w_{t+1} &= w_t - \\lambda \\frac{g_t}{\\sqrt{v_{t+1}} + \\epsilon}\n\nArgs:\n    learning_rate (float): The learning rate :math:`\\lambda`.\n    eps (float, optional): The term :math:`\\epsilon` added to the\n      denominator to improve numerical stability. Default: ``1e-8``"}
{"text": "Q: What is relu in mlx?\nA: mlx.nn.relu Applies the Rectified Linear Unit.\n\nSimply ``mx.maximum(x, 0)``."}
{"text": "Q: What is gelu_fast_approx in mlx?\nA: mlx.nn.layers.gelu_fast_approx A fast approximation to Gaussian Error Linear Unit.\n\nSee :func:`gelu` for the exact computation.\n\nThis function approximates ``gelu`` with a maximum absolute error :math:`<\n0.015` in the range :math:`[-6, 6]` using the following\n\n.. math::\n\n    x = x \\sigma\\left(1.773 x\\right)\n\nwhere :math:`\\sigma(\\cdot)` is the logistic sigmoid."}
{"text": "Q: What is ALiBi in mlx?\nA: mlx.nn.ALiBi Base class for building neural networks with MLX.\n\nAll the layers provided in :mod:`mlx.nn.layers` subclass this class and\nyour models should do the same.\n\nA ``Module`` can contain other ``Module`` instances or :class:`mlx.core.array`\ninstances in arbitrary nesting of python lists or dicts. The ``Module``\nthen allows recursively extracting all the :class:`mlx.core.array` instances\nusing :meth:`mlx.nn.Module.parameters`.\n\nIn addition, the ``Module`` has the concept of trainable and non trainable\nparameters (called \"frozen\"). When using :func:`mlx.nn.value_and_grad`\nthe gradients are returned only with respect to the trainable parameters.\nAll arrays in a module are trainable unless they are added in the \"frozen\"\nset by calling :meth:`freeze`.\n\n.. code-block:: python\n\n    import mlx.core as mx\n    import mlx.nn as nn\n\n    class MyMLP(nn.Module):\n        def __init__(self, in_dims: int, out_dims: int, hidden_dims: int = 16):\n            super().__init__()\n\n            self.in_proj = nn.Linear(in_dims, hidden_dims)\n            self.out_proj = nn.Linear(hidden_dims, out_dims)\n\n        def __call__(self, x):\n            x = self.in_proj(x)\n            x = mx.maximum(x, 0)\n            return self.out_proj(x)\n\n    model = MyMLP(2, 1)\n\n    # All the model parameters are created but since MLX is lazy by\n    # default, they are not evaluated yet. Calling `mx.eval` actually\n    # allocates memory and initializes the parameters.\n    mx.eval(model.parameters())\n\n    # Setting a parameter to a new value is as simply as accessing that\n    # parameter and assigning a new array to it.\n    model.in_proj.weight = model.in_proj.weight * 2\n    mx.eval(model.parameters())"}
{"text": "Q: What is GELU in mlx?\nA: mlx.nn.layers.GELU Applies the Gaussian Error Linear Units.\n\n.. math::\n    \\textrm{GELU}(x) = x * \\Phi(x)\n\nwhere :math:`\\Phi(x)` is the Gaussian CDF.\n\nHowever, if ``approx`` is set to 'precise' or 'fast' it applies\n\n.. math::\n    \\textrm{GELUApprox}(x) &= x * \\sigma\\left(1.60033 * x \\left(1 + 0.0433603 * x^2\\right)\\right) \\\\\n    \\textrm{GELUFast}(x) &= x * \\sigma\\left(1.773 * x\\right)\n\nrespectively.\n\nSee :func:`gelu`, :func:`gelu_approx` and :func:`gelu_fast_approx` for the\nfunctional equivalents and information regarding error bounds.\n\nArgs:\n    approx ('none' | 'precise' | 'fast'): Which approximation to gelu to use if any."}
{"text": "Q: What is Module in mlx?\nA: mlx.nn.losses.Module Base class for building neural networks with MLX.\n\nAll the layers provided in :mod:`mlx.nn.layers` subclass this class and\nyour models should do the same.\n\nA ``Module`` can contain other ``Module`` instances or :class:`mlx.core.array`\ninstances in arbitrary nesting of python lists or dicts. The ``Module``\nthen allows recursively extracting all the :class:`mlx.core.array` instances\nusing :meth:`mlx.nn.Module.parameters`.\n\nIn addition, the ``Module`` has the concept of trainable and non trainable\nparameters (called \"frozen\"). When using :func:`mlx.nn.value_and_grad`\nthe gradients are returned only with respect to the trainable parameters.\nAll arrays in a module are trainable unless they are added in the \"frozen\"\nset by calling :meth:`freeze`.\n\n.. code-block:: python\n\n    import mlx.core as mx\n    import mlx.nn as nn\n\n    class MyMLP(nn.Module):\n        def __init__(self, in_dims: int, out_dims: int, hidden_dims: int = 16):\n            super().__init__()\n\n            self.in_proj = nn.Linear(in_dims, hidden_dims)\n            self.out_proj = nn.Linear(hidden_dims, out_dims)\n\n        def __call__(self, x):\n            x = self.in_proj(x)\n            x = mx.maximum(x, 0)\n            return self.out_proj(x)\n\n    model = MyMLP(2, 1)\n\n    # All the model parameters are created but since MLX is lazy by\n    # default, they are not evaluated yet. Calling `mx.eval` actually\n    # allocates memory and initializes the parameters.\n    mx.eval(model.parameters())\n\n    # Setting a parameter to a new value is as simply as accessing that\n    # parameter and assigning a new array to it.\n    model.in_proj.weight = model.in_proj.weight * 2\n    mx.eval(model.parameters())"}
{"text": "Q: What is Module in mlx?\nA: mlx.nn.layers.linear.Module Base class for building neural networks with MLX.\n\nAll the layers provided in :mod:`mlx.nn.layers` subclass this class and\nyour models should do the same.\n\nA ``Module`` can contain other ``Module`` instances or :class:`mlx.core.array`\ninstances in arbitrary nesting of python lists or dicts. The ``Module``\nthen allows recursively extracting all the :class:`mlx.core.array` instances\nusing :meth:`mlx.nn.Module.parameters`.\n\nIn addition, the ``Module`` has the concept of trainable and non trainable\nparameters (called \"frozen\"). When using :func:`mlx.nn.value_and_grad`\nthe gradients are returned only with respect to the trainable parameters.\nAll arrays in a module are trainable unless they are added in the \"frozen\"\nset by calling :meth:`freeze`.\n\n.. code-block:: python\n\n    import mlx.core as mx\n    import mlx.nn as nn\n\n    class MyMLP(nn.Module):\n        def __init__(self, in_dims: int, out_dims: int, hidden_dims: int = 16):\n            super().__init__()\n\n            self.in_proj = nn.Linear(in_dims, hidden_dims)\n            self.out_proj = nn.Linear(hidden_dims, out_dims)\n\n        def __call__(self, x):\n            x = self.in_proj(x)\n            x = mx.maximum(x, 0)\n            return self.out_proj(x)\n\n    model = MyMLP(2, 1)\n\n    # All the model parameters are created but since MLX is lazy by\n    # default, they are not evaluated yet. Calling `mx.eval` actually\n    # allocates memory and initializes the parameters.\n    mx.eval(model.parameters())\n\n    # Setting a parameter to a new value is as simply as accessing that\n    # parameter and assigning a new array to it.\n    model.in_proj.weight = model.in_proj.weight * 2\n    mx.eval(model.parameters())"}
{"text": "Q: What is array in mlx?\nA: mlx.core.array An N-dimensional array object."}
{"text": "Q: What is Module in mlx?\nA: mlx.nn.layers.base.Module Base class for building neural networks with MLX.\n\nAll the layers provided in :mod:`mlx.nn.layers` subclass this class and\nyour models should do the same.\n\nA ``Module`` can contain other ``Module`` instances or :class:`mlx.core.array`\ninstances in arbitrary nesting of python lists or dicts. The ``Module``\nthen allows recursively extracting all the :class:`mlx.core.array` instances\nusing :meth:`mlx.nn.Module.parameters`.\n\nIn addition, the ``Module`` has the concept of trainable and non trainable\nparameters (called \"frozen\"). When using :func:`mlx.nn.value_and_grad`\nthe gradients are returned only with respect to the trainable parameters.\nAll arrays in a module are trainable unless they are added in the \"frozen\"\nset by calling :meth:`freeze`.\n\n.. code-block:: python\n\n    import mlx.core as mx\n    import mlx.nn as nn\n\n    class MyMLP(nn.Module):\n        def __init__(self, in_dims: int, out_dims: int, hidden_dims: int = 16):\n            super().__init__()\n\n            self.in_proj = nn.Linear(in_dims, hidden_dims)\n            self.out_proj = nn.Linear(hidden_dims, out_dims)\n\n        def __call__(self, x):\n            x = self.in_proj(x)\n            x = mx.maximum(x, 0)\n            return self.out_proj(x)\n\n    model = MyMLP(2, 1)\n\n    # All the model parameters are created but since MLX is lazy by\n    # default, they are not evaluated yet. Calling `mx.eval` actually\n    # allocates memory and initializes the parameters.\n    mx.eval(model.parameters())\n\n    # Setting a parameter to a new value is as simply as accessing that\n    # parameter and assigning a new array to it.\n    model.in_proj.weight = model.in_proj.weight * 2\n    mx.eval(model.parameters())"}
{"text": "Q: What is RMSNorm in mlx?\nA: mlx.nn.RMSNorm Applies Root Mean Square normalization [1] to the inputs.\n\nComputes\n\n..  math::\n\n    y = \\frac{x}{\\sqrt{E[x^2] + \\epsilon}} \\gamma\n\nwhere :math:`\\gamma` is a learned per feature dimension parameter initialized at\n1.\n\n[1]: https://arxiv.org/abs/1910.07467\n\nArgs:\n    dims (int): The feature dimension of the input to normalize over\n    eps (float): A small additive constant for numerical stability"}
{"text": "Q: What is nll_loss in mlx?\nA: mlx.nn.losses.nll_loss Computes the negative log likelihood loss.\n\nArgs:\n    inputs (array): The predicted distribution in log space.\n    targets (array): The target values.\n    axis (int, optional): The distribution axis. Default: ``-1``.\n    reduction (str, optional): Specifies the reduction to apply to the output:\n      ``'none'`` | ``'mean'`` | ``'sum'``. Default: ``'none'``.\n\nReturns:\n    array: The computed NLL loss."}
{"text": "Q: What is Module in mlx?\nA: mlx.nn.layers.transformer.Module Base class for building neural networks with MLX.\n\nAll the layers provided in :mod:`mlx.nn.layers` subclass this class and\nyour models should do the same.\n\nA ``Module`` can contain other ``Module`` instances or :class:`mlx.core.array`\ninstances in arbitrary nesting of python lists or dicts. The ``Module``\nthen allows recursively extracting all the :class:`mlx.core.array` instances\nusing :meth:`mlx.nn.Module.parameters`.\n\nIn addition, the ``Module`` has the concept of trainable and non trainable\nparameters (called \"frozen\"). When using :func:`mlx.nn.value_and_grad`\nthe gradients are returned only with respect to the trainable parameters.\nAll arrays in a module are trainable unless they are added in the \"frozen\"\nset by calling :meth:`freeze`.\n\n.. code-block:: python\n\n    import mlx.core as mx\n    import mlx.nn as nn\n\n    class MyMLP(nn.Module):\n        def __init__(self, in_dims: int, out_dims: int, hidden_dims: int = 16):\n            super().__init__()\n\n            self.in_proj = nn.Linear(in_dims, hidden_dims)\n            self.out_proj = nn.Linear(hidden_dims, out_dims)\n\n        def __call__(self, x):\n            x = self.in_proj(x)\n            x = mx.maximum(x, 0)\n            return self.out_proj(x)\n\n    model = MyMLP(2, 1)\n\n    # All the model parameters are created but since MLX is lazy by\n    # default, they are not evaluated yet. Calling `mx.eval` actually\n    # allocates memory and initializes the parameters.\n    mx.eval(model.parameters())\n\n    # Setting a parameter to a new value is as simply as accessing that\n    # parameter and assigning a new array to it.\n    model.in_proj.weight = model.in_proj.weight * 2\n    mx.eval(model.parameters())"}
{"text": "Q: What is TransformerEncoder in mlx?\nA: mlx.nn.TransformerEncoder Base class for building neural networks with MLX.\n\nAll the layers provided in :mod:`mlx.nn.layers` subclass this class and\nyour models should do the same.\n\nA ``Module`` can contain other ``Module`` instances or :class:`mlx.core.array`\ninstances in arbitrary nesting of python lists or dicts. The ``Module``\nthen allows recursively extracting all the :class:`mlx.core.array` instances\nusing :meth:`mlx.nn.Module.parameters`.\n\nIn addition, the ``Module`` has the concept of trainable and non trainable\nparameters (called \"frozen\"). When using :func:`mlx.nn.value_and_grad`\nthe gradients are returned only with respect to the trainable parameters.\nAll arrays in a module are trainable unless they are added in the \"frozen\"\nset by calling :meth:`freeze`.\n\n.. code-block:: python\n\n    import mlx.core as mx\n    import mlx.nn as nn\n\n    class MyMLP(nn.Module):\n        def __init__(self, in_dims: int, out_dims: int, hidden_dims: int = 16):\n            super().__init__()\n\n            self.in_proj = nn.Linear(in_dims, hidden_dims)\n            self.out_proj = nn.Linear(hidden_dims, out_dims)\n\n        def __call__(self, x):\n            x = self.in_proj(x)\n            x = mx.maximum(x, 0)\n            return self.out_proj(x)\n\n    model = MyMLP(2, 1)\n\n    # All the model parameters are created but since MLX is lazy by\n    # default, they are not evaluated yet. Calling `mx.eval` actually\n    # allocates memory and initializes the parameters.\n    mx.eval(model.parameters())\n\n    # Setting a parameter to a new value is as simply as accessing that\n    # parameter and assigning a new array to it.\n    model.in_proj.weight = model.in_proj.weight * 2\n    mx.eval(model.parameters())"}
{"text": "Q: What is relu in mlx?\nA: mlx.nn.layers.activations.relu Applies the Rectified Linear Unit.\n\nSimply ``mx.maximum(x, 0)``."}
{"text": "Q: What is Module in mlx?\nA: mlx.nn.Module Base class for building neural networks with MLX.\n\nAll the layers provided in :mod:`mlx.nn.layers` subclass this class and\nyour models should do the same.\n\nA ``Module`` can contain other ``Module`` instances or :class:`mlx.core.array`\ninstances in arbitrary nesting of python lists or dicts. The ``Module``\nthen allows recursively extracting all the :class:`mlx.core.array` instances\nusing :meth:`mlx.nn.Module.parameters`.\n\nIn addition, the ``Module`` has the concept of trainable and non trainable\nparameters (called \"frozen\"). When using :func:`mlx.nn.value_and_grad`\nthe gradients are returned only with respect to the trainable parameters.\nAll arrays in a module are trainable unless they are added in the \"frozen\"\nset by calling :meth:`freeze`.\n\n.. code-block:: python\n\n    import mlx.core as mx\n    import mlx.nn as nn\n\n    class MyMLP(nn.Module):\n        def __init__(self, in_dims: int, out_dims: int, hidden_dims: int = 16):\n            super().__init__()\n\n            self.in_proj = nn.Linear(in_dims, hidden_dims)\n            self.out_proj = nn.Linear(hidden_dims, out_dims)\n\n        def __call__(self, x):\n            x = self.in_proj(x)\n            x = mx.maximum(x, 0)\n            return self.out_proj(x)\n\n    model = MyMLP(2, 1)\n\n    # All the model parameters are created but since MLX is lazy by\n    # default, they are not evaluated yet. Calling `mx.eval` actually\n    # allocates memory and initializes the parameters.\n    mx.eval(model.parameters())\n\n    # Setting a parameter to a new value is as simply as accessing that\n    # parameter and assigning a new array to it.\n    model.in_proj.weight = model.in_proj.weight * 2\n    mx.eval(model.parameters())"}
{"text": "Q: What is QuantizedLinear in mlx?\nA: mlx.nn.QuantizedLinear Applies an affine transformation to the input using a quantized weight matrix.\n\nIt is the quantized equivalent of :class:`mlx.nn.Linear`. For now its\nparameters are frozen and will not be included in any gradient computation\nbut this will probably change in the future.\n\nQuantizedLinear also provides two useful classmethods to convert linear\nlayers to QuantizedLinear layers.\n\n- :meth:`from_linear` returns a QuantizedLinear layer that applies the same\n  linear transformation up to the quantization error.\n- :meth:`quantize_module` swaps all the linear layers of the passed module\n  with QuantizedLinear ones.\n\nArgs:\n    input_dims (int): The dimensionality of the input features\n    output_dims (int): The dimensionality of the output features\n    bias (bool, optional): If set to ``False`` then the layer will not use\n        a bias. (default: True).\n    group_size (int, optional): The group size to use for the quantized\n        weight. See :func:`~mlx.core.quantize`. (default: 64)\n    bits (int, optional): The bit width to use for the quantized weight.\n        See :func:`~mlx.core.quantize`. (default: 4)"}
{"text": "Q: What is Module in mlx?\nA: mlx.nn.layers.convolution.Module Base class for building neural networks with MLX.\n\nAll the layers provided in :mod:`mlx.nn.layers` subclass this class and\nyour models should do the same.\n\nA ``Module`` can contain other ``Module`` instances or :class:`mlx.core.array`\ninstances in arbitrary nesting of python lists or dicts. The ``Module``\nthen allows recursively extracting all the :class:`mlx.core.array` instances\nusing :meth:`mlx.nn.Module.parameters`.\n\nIn addition, the ``Module`` has the concept of trainable and non trainable\nparameters (called \"frozen\"). When using :func:`mlx.nn.value_and_grad`\nthe gradients are returned only with respect to the trainable parameters.\nAll arrays in a module are trainable unless they are added in the \"frozen\"\nset by calling :meth:`freeze`.\n\n.. code-block:: python\n\n    import mlx.core as mx\n    import mlx.nn as nn\n\n    class MyMLP(nn.Module):\n        def __init__(self, in_dims: int, out_dims: int, hidden_dims: int = 16):\n            super().__init__()\n\n            self.in_proj = nn.Linear(in_dims, hidden_dims)\n            self.out_proj = nn.Linear(hidden_dims, out_dims)\n\n        def __call__(self, x):\n            x = self.in_proj(x)\n            x = mx.maximum(x, 0)\n            return self.out_proj(x)\n\n    model = MyMLP(2, 1)\n\n    # All the model parameters are created but since MLX is lazy by\n    # default, they are not evaluated yet. Calling `mx.eval` actually\n    # allocates memory and initializes the parameters.\n    mx.eval(model.parameters())\n\n    # Setting a parameter to a new value is as simply as accessing that\n    # parameter and assigning a new array to it.\n    model.in_proj.weight = model.in_proj.weight * 2\n    mx.eval(model.parameters())"}
{"text": "Q: What is LayerNorm in mlx?\nA: mlx.nn.layers.normalization.LayerNorm Applies layer normalization [1] on the inputs.\n\nComputes\n\n.. math::\n\n    y = \\frac{x - E[x]}{\\sqrt{Var[x]} + \\epsilon} \\gamma + \\beta,\n\nwhere :math:`\\gamma` and :math:`\\beta` are learned per feature dimension\nparameters initialized at 1 and 0 respectively.\n\n[1]: https://arxiv.org/abs/1607.06450\n\nArgs:\n    dims (int): The feature dimension of the input to normalize over\n    eps (float): A small additive constant for numerical stability\n    affine (bool): If True learn an affine transform to apply after the\n        normalization"}
{"text": "Q: What is TransformerEncoderLayer in mlx?\nA: mlx.nn.layers.transformer.TransformerEncoderLayer Base class for building neural networks with MLX.\n\nAll the layers provided in :mod:`mlx.nn.layers` subclass this class and\nyour models should do the same.\n\nA ``Module`` can contain other ``Module`` instances or :class:`mlx.core.array`\ninstances in arbitrary nesting of python lists or dicts. The ``Module``\nthen allows recursively extracting all the :class:`mlx.core.array` instances\nusing :meth:`mlx.nn.Module.parameters`.\n\nIn addition, the ``Module`` has the concept of trainable and non trainable\nparameters (called \"frozen\"). When using :func:`mlx.nn.value_and_grad`\nthe gradients are returned only with respect to the trainable parameters.\nAll arrays in a module are trainable unless they are added in the \"frozen\"\nset by calling :meth:`freeze`.\n\n.. code-block:: python\n\n    import mlx.core as mx\n    import mlx.nn as nn\n\n    class MyMLP(nn.Module):\n        def __init__(self, in_dims: int, out_dims: int, hidden_dims: int = 16):\n            super().__init__()\n\n            self.in_proj = nn.Linear(in_dims, hidden_dims)\n            self.out_proj = nn.Linear(hidden_dims, out_dims)\n\n        def __call__(self, x):\n            x = self.in_proj(x)\n            x = mx.maximum(x, 0)\n            return self.out_proj(x)\n\n    model = MyMLP(2, 1)\n\n    # All the model parameters are created but since MLX is lazy by\n    # default, they are not evaluated yet. Calling `mx.eval` actually\n    # allocates memory and initializes the parameters.\n    mx.eval(model.parameters())\n\n    # Setting a parameter to a new value is as simply as accessing that\n    # parameter and assigning a new array to it.\n    model.in_proj.weight = model.in_proj.weight * 2\n    mx.eval(model.parameters())"}
{"text": "Q: What is LogSigmoid in mlx?\nA: mlx.nn.layers.LogSigmoid Applies the Log Sigmoid function.\n\nApplies :math:`\\log(\\sigma(x)) = -\\log(1 + e^{-x})` element wise."}
{"text": "Q: What is SiLU in mlx?\nA: mlx.nn.layers.SiLU Applies the Sigmoid Linear Unit. Also known as Swish.\n\nApplies :math:`x \\sigma(x)` element wise, where :math:`\\sigma(\\cdot)` is\nthe logistic sigmoid."}
{"text": "Q: What is Module in mlx?\nA: mlx.nn.layers.positional_encoding.Module Base class for building neural networks with MLX.\n\nAll the layers provided in :mod:`mlx.nn.layers` subclass this class and\nyour models should do the same.\n\nA ``Module`` can contain other ``Module`` instances or :class:`mlx.core.array`\ninstances in arbitrary nesting of python lists or dicts. The ``Module``\nthen allows recursively extracting all the :class:`mlx.core.array` instances\nusing :meth:`mlx.nn.Module.parameters`.\n\nIn addition, the ``Module`` has the concept of trainable and non trainable\nparameters (called \"frozen\"). When using :func:`mlx.nn.value_and_grad`\nthe gradients are returned only with respect to the trainable parameters.\nAll arrays in a module are trainable unless they are added in the \"frozen\"\nset by calling :meth:`freeze`.\n\n.. code-block:: python\n\n    import mlx.core as mx\n    import mlx.nn as nn\n\n    class MyMLP(nn.Module):\n        def __init__(self, in_dims: int, out_dims: int, hidden_dims: int = 16):\n            super().__init__()\n\n            self.in_proj = nn.Linear(in_dims, hidden_dims)\n            self.out_proj = nn.Linear(hidden_dims, out_dims)\n\n        def __call__(self, x):\n            x = self.in_proj(x)\n            x = mx.maximum(x, 0)\n            return self.out_proj(x)\n\n    model = MyMLP(2, 1)\n\n    # All the model parameters are created but since MLX is lazy by\n    # default, they are not evaluated yet. Calling `mx.eval` actually\n    # allocates memory and initializes the parameters.\n    mx.eval(model.parameters())\n\n    # Setting a parameter to a new value is as simply as accessing that\n    # parameter and assigning a new array to it.\n    model.in_proj.weight = model.in_proj.weight * 2\n    mx.eval(model.parameters())"}
{"text": "Q: What is tree_flatten in mlx?\nA: mlx.nn.layers.quantized.tree_flatten Flattens a python tree to a list of key, value tuples.\n\nThe keys are using the dot notation to define trees of arbitrary depth and\ncomplexity.\n\n.. code-block:: python\n\n    from mlx.utils import tree_flatten\n\n    print(tree_flatten([[[0]]]))\n    # [(\"0.0.0\", 0)]\n\n    print(tree_flatten([[[0]]], \".hello\"))\n    # [(\"hello.0.0.0\", 0)]\n\n.. note::\n   Dictionaries should have keys that are valid python identifiers.\n\nArgs:\n    tree (Any): The python tree to be flattened.\n    prefix (str): A prefix to use for the keys. The first character is\n        always discarded.\n    is_leaf (Callable): An optional callable that returns True if the\n        passed object is considered a leaf or False otherwise.\n\nReturns:\n    List[Tuple[str, Any]]: The flat representation of the python tree."}
{"text": "Q: What is Mish in mlx?\nA: mlx.nn.Mish Applies the Mish function, element-wise.\nMish: A Self Regularized Non-Monotonic Neural Activation Function.\n\nReference: https://arxiv.org/abs/1908.08681\n\n.. math::\n    \\text{Mish}(x) = x * \\text{Tanh}(\\text{Softplus}(x))"}
{"text": "Q: What is Softplus in mlx?\nA: mlx.nn.layers.Softplus Applies the Softplus function.\n\nApplies :math:`\\log(1 + \\exp(x))` element wise."}
{"text": "Q: What is PReLU in mlx?\nA: mlx.nn.layers.activations.PReLU Base class for building neural networks with MLX.\n\nAll the layers provided in :mod:`mlx.nn.layers` subclass this class and\nyour models should do the same.\n\nA ``Module`` can contain other ``Module`` instances or :class:`mlx.core.array`\ninstances in arbitrary nesting of python lists or dicts. The ``Module``\nthen allows recursively extracting all the :class:`mlx.core.array` instances\nusing :meth:`mlx.nn.Module.parameters`.\n\nIn addition, the ``Module`` has the concept of trainable and non trainable\nparameters (called \"frozen\"). When using :func:`mlx.nn.value_and_grad`\nthe gradients are returned only with respect to the trainable parameters.\nAll arrays in a module are trainable unless they are added in the \"frozen\"\nset by calling :meth:`freeze`.\n\n.. code-block:: python\n\n    import mlx.core as mx\n    import mlx.nn as nn\n\n    class MyMLP(nn.Module):\n        def __init__(self, in_dims: int, out_dims: int, hidden_dims: int = 16):\n            super().__init__()\n\n            self.in_proj = nn.Linear(in_dims, hidden_dims)\n            self.out_proj = nn.Linear(hidden_dims, out_dims)\n\n        def __call__(self, x):\n            x = self.in_proj(x)\n            x = mx.maximum(x, 0)\n            return self.out_proj(x)\n\n    model = MyMLP(2, 1)\n\n    # All the model parameters are created but since MLX is lazy by\n    # default, they are not evaluated yet. Calling `mx.eval` actually\n    # allocates memory and initializes the parameters.\n    mx.eval(model.parameters())\n\n    # Setting a parameter to a new value is as simply as accessing that\n    # parameter and assigning a new array to it.\n    model.in_proj.weight = model.in_proj.weight * 2\n    mx.eval(model.parameters())"}
{"text": "Q: What is selu in mlx?\nA: mlx.nn.layers.selu Applies the Scaled Exponential Linear Unit.\n\n.. math::\n    \\text{selu}(x) = \\begin{cases}\n    \\lambda x & \\text{if } x > 0 \\\\\n    \\lambda \\alpha (\\exp(x) - 1) & \\text{if } x \\leq 0\n    \\end{cases}\n\nwhere :math:`\\lambda = 1.0507` and :math:`\\alpha = 1.67326`.\n\nSee also :func:`elu`."}
{"text": "Q: What is leaky_relu in mlx?\nA: mlx.nn.leaky_relu Applies the Leaky Rectified Linear Unit.\n\nSimply ``mx.maximum(negative_slope * x, x)``."}
{"text": "Q: What is Sequential in mlx?\nA: mlx.nn.layers.Sequential A layer that calls the passed callables in order.\n\nWe can pass either modules or plain callables to the Sequential module. If\nour functions have learnable parameters they should be implemented as\n``nn.Module`` instances.\n\nArgs:\n    modules (tuple of Callables): The modules to call in order"}
{"text": "Q: What is SELU in mlx?\nA: mlx.nn.layers.activations.SELU Applies the Scaled Exponential Linear Unit.\n\n.. math::\n    \\text{selu}(x) = \\begin{cases}\n    \\lambda x & \\text{if } x > 0 \\\\\n    \\lambda \\alpha (\\exp(x) - 1) & \\text{if } x \\leq 0\n    \\end{cases}\n\nwhere :math:`\\lambda = 1.0507` and :math:`\\alpha = 1.67326`.\n\nSee also :func:`elu`."}
{"text": "Q: What is GELU in mlx?\nA: mlx.nn.layers.activations.GELU Applies the Gaussian Error Linear Units.\n\n.. math::\n    \\textrm{GELU}(x) = x * \\Phi(x)\n\nwhere :math:`\\Phi(x)` is the Gaussian CDF.\n\nHowever, if ``approx`` is set to 'precise' or 'fast' it applies\n\n.. math::\n    \\textrm{GELUApprox}(x) &= x * \\sigma\\left(1.60033 * x \\left(1 + 0.0433603 * x^2\\right)\\right) \\\\\n    \\textrm{GELUFast}(x) &= x * \\sigma\\left(1.773 * x\\right)\n\nrespectively.\n\nSee :func:`gelu`, :func:`gelu_approx` and :func:`gelu_fast_approx` for the\nfunctional equivalents and information regarding error bounds.\n\nArgs:\n    approx ('none' | 'precise' | 'fast'): Which approximation to gelu to use if any."}
{"text": "Q: What is mish in mlx?\nA: mlx.nn.layers.mish Applies the Mish function, element-wise.\nMish: A Self Regularized Non-Monotonic Neural Activation Function.\n\nReference: https://arxiv.org/abs/1908.08681\n\n.. math::\n    \\text{Mish}(x) = x * \\text{Tanh}(\\text{Softplus}(x))"}
{"text": "Q: What is ReLU6 in mlx?\nA: mlx.nn.ReLU6 Applies the Rectified Linear Unit 6.\n\nApplies :math:`\\min(\\max(x, 0), 6)` element wise."}
{"text": "Q: What is Module in mlx?\nA: mlx.nn.layers.normalization.Module Base class for building neural networks with MLX.\n\nAll the layers provided in :mod:`mlx.nn.layers` subclass this class and\nyour models should do the same.\n\nA ``Module`` can contain other ``Module`` instances or :class:`mlx.core.array`\ninstances in arbitrary nesting of python lists or dicts. The ``Module``\nthen allows recursively extracting all the :class:`mlx.core.array` instances\nusing :meth:`mlx.nn.Module.parameters`.\n\nIn addition, the ``Module`` has the concept of trainable and non trainable\nparameters (called \"frozen\"). When using :func:`mlx.nn.value_and_grad`\nthe gradients are returned only with respect to the trainable parameters.\nAll arrays in a module are trainable unless they are added in the \"frozen\"\nset by calling :meth:`freeze`.\n\n.. code-block:: python\n\n    import mlx.core as mx\n    import mlx.nn as nn\n\n    class MyMLP(nn.Module):\n        def __init__(self, in_dims: int, out_dims: int, hidden_dims: int = 16):\n            super().__init__()\n\n            self.in_proj = nn.Linear(in_dims, hidden_dims)\n            self.out_proj = nn.Linear(hidden_dims, out_dims)\n\n        def __call__(self, x):\n            x = self.in_proj(x)\n            x = mx.maximum(x, 0)\n            return self.out_proj(x)\n\n    model = MyMLP(2, 1)\n\n    # All the model parameters are created but since MLX is lazy by\n    # default, they are not evaluated yet. Calling `mx.eval` actually\n    # allocates memory and initializes the parameters.\n    mx.eval(model.parameters())\n\n    # Setting a parameter to a new value is as simply as accessing that\n    # parameter and assigning a new array to it.\n    model.in_proj.weight = model.in_proj.weight * 2\n    mx.eval(model.parameters())"}
{"text": "Q: What is Step in mlx?\nA: mlx.nn.Step Applies the Step Activation Function.\n\nThis function implements a binary step activation, where the output is set\nto 1 if the input is greater than a specified threshold, and 0 otherwise.\n\n.. math::\n    \\text{step}(x) = \\begin{cases}\n    0 & \\text{if } x < \\text{threshold} \\\\\n    1 & \\text{if } x \\geq \\text{threshold}\n    \\end{cases}\n\nArgs:\n    threshold: The value to threshold at."}
{"text": "Q: What is mish in mlx?\nA: mlx.nn.mish Applies the Mish function, element-wise.\nMish: A Self Regularized Non-Monotonic Neural Activation Function.\n\nReference: https://arxiv.org/abs/1908.08681\n\n.. math::\n    \\text{Mish}(x) = x * \\text{Tanh}(\\text{Softplus}(x))"}
{"text": "Q: What is RMSprop in mlx?\nA: mlx.optimizers.RMSprop Implementation of the RMSprop optimizer [1].\n\n[1]: Tieleman, T. and Hinton, G. 2012. Lecture 6.5-rmsprop, coursera: Neural networks for machine learning\n\n.. math::\n\n    v_{t+1} &= \\alpha v_t + (1 - \\alpha) g_t^2 \\\\\n    w_{t+1} &= w_t - \\lambda \\frac{g_t}{\\sqrt{v_{t+1}} + \\epsilon}\n\nArgs:\n    learning_rate (float): The learning rate :math:`\\lambda`.\n    alpha (float, optional): The smoothing constant :math:`\\alpha`.\n      Default: ``0.99``\n    eps (float, optional): The term :math:`\\epsilon` added to the denominator\n      to improve numerical stability. Default: ``1e-8``"}
{"text": "Q: What is SiLU in mlx?\nA: mlx.nn.layers.activations.SiLU Applies the Sigmoid Linear Unit. Also known as Swish.\n\nApplies :math:`x \\sigma(x)` element wise, where :math:`\\sigma(\\cdot)` is\nthe logistic sigmoid."}
{"text": "Q: What is TransformerEncoder in mlx?\nA: mlx.nn.layers.TransformerEncoder Base class for building neural networks with MLX.\n\nAll the layers provided in :mod:`mlx.nn.layers` subclass this class and\nyour models should do the same.\n\nA ``Module`` can contain other ``Module`` instances or :class:`mlx.core.array`\ninstances in arbitrary nesting of python lists or dicts. The ``Module``\nthen allows recursively extracting all the :class:`mlx.core.array` instances\nusing :meth:`mlx.nn.Module.parameters`.\n\nIn addition, the ``Module`` has the concept of trainable and non trainable\nparameters (called \"frozen\"). When using :func:`mlx.nn.value_and_grad`\nthe gradients are returned only with respect to the trainable parameters.\nAll arrays in a module are trainable unless they are added in the \"frozen\"\nset by calling :meth:`freeze`.\n\n.. code-block:: python\n\n    import mlx.core as mx\n    import mlx.nn as nn\n\n    class MyMLP(nn.Module):\n        def __init__(self, in_dims: int, out_dims: int, hidden_dims: int = 16):\n            super().__init__()\n\n            self.in_proj = nn.Linear(in_dims, hidden_dims)\n            self.out_proj = nn.Linear(hidden_dims, out_dims)\n\n        def __call__(self, x):\n            x = self.in_proj(x)\n            x = mx.maximum(x, 0)\n            return self.out_proj(x)\n\n    model = MyMLP(2, 1)\n\n    # All the model parameters are created but since MLX is lazy by\n    # default, they are not evaluated yet. Calling `mx.eval` actually\n    # allocates memory and initializes the parameters.\n    mx.eval(model.parameters())\n\n    # Setting a parameter to a new value is as simply as accessing that\n    # parameter and assigning a new array to it.\n    model.in_proj.weight = model.in_proj.weight * 2\n    mx.eval(model.parameters())"}
{"text": "Q: What is TransformerEncoder in mlx?\nA: mlx.nn.layers.transformer.TransformerEncoder Base class for building neural networks with MLX.\n\nAll the layers provided in :mod:`mlx.nn.layers` subclass this class and\nyour models should do the same.\n\nA ``Module`` can contain other ``Module`` instances or :class:`mlx.core.array`\ninstances in arbitrary nesting of python lists or dicts. The ``Module``\nthen allows recursively extracting all the :class:`mlx.core.array` instances\nusing :meth:`mlx.nn.Module.parameters`.\n\nIn addition, the ``Module`` has the concept of trainable and non trainable\nparameters (called \"frozen\"). When using :func:`mlx.nn.value_and_grad`\nthe gradients are returned only with respect to the trainable parameters.\nAll arrays in a module are trainable unless they are added in the \"frozen\"\nset by calling :meth:`freeze`.\n\n.. code-block:: python\n\n    import mlx.core as mx\n    import mlx.nn as nn\n\n    class MyMLP(nn.Module):\n        def __init__(self, in_dims: int, out_dims: int, hidden_dims: int = 16):\n            super().__init__()\n\n            self.in_proj = nn.Linear(in_dims, hidden_dims)\n            self.out_proj = nn.Linear(hidden_dims, out_dims)\n\n        def __call__(self, x):\n            x = self.in_proj(x)\n            x = mx.maximum(x, 0)\n            return self.out_proj(x)\n\n    model = MyMLP(2, 1)\n\n    # All the model parameters are created but since MLX is lazy by\n    # default, they are not evaluated yet. Calling `mx.eval` actually\n    # allocates memory and initializes the parameters.\n    mx.eval(model.parameters())\n\n    # Setting a parameter to a new value is as simply as accessing that\n    # parameter and assigning a new array to it.\n    model.in_proj.weight = model.in_proj.weight * 2\n    mx.eval(model.parameters())"}
{"text": "Q: What is softplus in mlx?\nA: mlx.nn.layers.activations.softplus Applies the Softplus function.\n\nApplies :math:`\\log(1 + \\exp(x))` element wise."}
{"text": "Q: What is Adam in mlx?\nA: mlx.optimizers.Adam Implementation of the Adam optimizer [1].\n\nOur Adam implementation follows the original paper and omits the bias\ncorrection in the first and second moment estimates. In detail,\n\n[1]: Kingma, D.P. and Ba, J., 2015. Adam: A method for stochastic\noptimization. ICLR 2015.\n\n.. math::\n\n    m_{t+1} &= \\beta_1 m_t + (1 - \\beta_1) g_t \\\\\n    v_{t+1} &= \\beta_2 v_t + (1 - \\beta_2) g_t^2 \\\\\n    w_{t+1} &= w_t - \\lambda \\frac{m_{t+1}}{\\sqrt{v_{t+1} + \\epsilon}}\n\nArgs:\n    learning_rate (float): The learning rate :math:`\\lambda`.\n    betas (Tuple[float, float], optional): The coefficients\n      :math:`(\\beta_1, \\beta_2)` used for computing running averages of the\n      gradient and its square. Default: ``(0.9, 0.999)``\n    eps (float, optional): The term :math:`\\epsilon` added to the\n      denominator to improve numerical stability. Default: ``1e-8``"}
{"text": "Q: What is Module in mlx?\nA: mlx.nn.layers.quantized.Module Base class for building neural networks with MLX.\n\nAll the layers provided in :mod:`mlx.nn.layers` subclass this class and\nyour models should do the same.\n\nA ``Module`` can contain other ``Module`` instances or :class:`mlx.core.array`\ninstances in arbitrary nesting of python lists or dicts. The ``Module``\nthen allows recursively extracting all the :class:`mlx.core.array` instances\nusing :meth:`mlx.nn.Module.parameters`.\n\nIn addition, the ``Module`` has the concept of trainable and non trainable\nparameters (called \"frozen\"). When using :func:`mlx.nn.value_and_grad`\nthe gradients are returned only with respect to the trainable parameters.\nAll arrays in a module are trainable unless they are added in the \"frozen\"\nset by calling :meth:`freeze`.\n\n.. code-block:: python\n\n    import mlx.core as mx\n    import mlx.nn as nn\n\n    class MyMLP(nn.Module):\n        def __init__(self, in_dims: int, out_dims: int, hidden_dims: int = 16):\n            super().__init__()\n\n            self.in_proj = nn.Linear(in_dims, hidden_dims)\n            self.out_proj = nn.Linear(hidden_dims, out_dims)\n\n        def __call__(self, x):\n            x = self.in_proj(x)\n            x = mx.maximum(x, 0)\n            return self.out_proj(x)\n\n    model = MyMLP(2, 1)\n\n    # All the model parameters are created but since MLX is lazy by\n    # default, they are not evaluated yet. Calling `mx.eval` actually\n    # allocates memory and initializes the parameters.\n    mx.eval(model.parameters())\n\n    # Setting a parameter to a new value is as simply as accessing that\n    # parameter and assigning a new array to it.\n    model.in_proj.weight = model.in_proj.weight * 2\n    mx.eval(model.parameters())"}
{"text": "Q: What is gelu_approx in mlx?\nA: mlx.nn.layers.gelu_approx An approximation to Gaussian Error Linear Unit.\n\nSee :func:`gelu` for the exact computation.\n\nThis function approximates ``gelu`` with a maximum absolute error :math:`<\n0.0003` in the range :math:`[-6, 6]` using the following\n\n.. math::\n\n    x = x \\sigma\\left(1.60033 x \\left(1 + 0.0433603 x^2\\right)\\right)\n\nwhere :math:`\\sigma(\\cdot)` is the logistic sigmoid."}
{"text": "Q: What is gelu_fast_approx in mlx?\nA: mlx.nn.gelu_fast_approx A fast approximation to Gaussian Error Linear Unit.\n\nSee :func:`gelu` for the exact computation.\n\nThis function approximates ``gelu`` with a maximum absolute error :math:`<\n0.015` in the range :math:`[-6, 6]` using the following\n\n.. math::\n\n    x = x \\sigma\\left(1.773 x\\right)\n\nwhere :math:`\\sigma(\\cdot)` is the logistic sigmoid."}
{"text": "Q: What is ReLU6 in mlx?\nA: mlx.nn.layers.activations.ReLU6 Applies the Rectified Linear Unit 6.\n\nApplies :math:`\\min(\\max(x, 0), 6)` element wise."}
{"text": "Q: What is GELU in mlx?\nA: mlx.nn.GELU Applies the Gaussian Error Linear Units.\n\n.. math::\n    \\textrm{GELU}(x) = x * \\Phi(x)\n\nwhere :math:`\\Phi(x)` is the Gaussian CDF.\n\nHowever, if ``approx`` is set to 'precise' or 'fast' it applies\n\n.. math::\n    \\textrm{GELUApprox}(x) &= x * \\sigma\\left(1.60033 * x \\left(1 + 0.0433603 * x^2\\right)\\right) \\\\\n    \\textrm{GELUFast}(x) &= x * \\sigma\\left(1.773 * x\\right)\n\nrespectively.\n\nSee :func:`gelu`, :func:`gelu_approx` and :func:`gelu_fast_approx` for the\nfunctional equivalents and information regarding error bounds.\n\nArgs:\n    approx ('none' | 'precise' | 'fast'): Which approximation to gelu to use if any."}
{"text": "Q: What is Sequential in mlx?\nA: mlx.nn.Sequential A layer that calls the passed callables in order.\n\nWe can pass either modules or plain callables to the Sequential module. If\nour functions have learnable parameters they should be implemented as\n``nn.Module`` instances.\n\nArgs:\n    modules (tuple of Callables): The modules to call in order"}
{"text": "Q: What is CELU in mlx?\nA: mlx.nn.layers.CELU Applies the Continuously Differentiable Exponential Linear Unit.\n    Applies :math:`\\max(0, x) + \\min(0, \\alpha * (\\exp(x / \\alpha) - 1))`\n    element wise.\n\nSee :func:`celu`, for the functional equivalent.\n\nArgs:\n    alpha: the :math:`\\alpha` value for the CELU formulation. Default: 1.0"}
{"text": "Q: What is Module in mlx?\nA: mlx.nn.layers.dropout.Module Base class for building neural networks with MLX.\n\nAll the layers provided in :mod:`mlx.nn.layers` subclass this class and\nyour models should do the same.\n\nA ``Module`` can contain other ``Module`` instances or :class:`mlx.core.array`\ninstances in arbitrary nesting of python lists or dicts. The ``Module``\nthen allows recursively extracting all the :class:`mlx.core.array` instances\nusing :meth:`mlx.nn.Module.parameters`.\n\nIn addition, the ``Module`` has the concept of trainable and non trainable\nparameters (called \"frozen\"). When using :func:`mlx.nn.value_and_grad`\nthe gradients are returned only with respect to the trainable parameters.\nAll arrays in a module are trainable unless they are added in the \"frozen\"\nset by calling :meth:`freeze`.\n\n.. code-block:: python\n\n    import mlx.core as mx\n    import mlx.nn as nn\n\n    class MyMLP(nn.Module):\n        def __init__(self, in_dims: int, out_dims: int, hidden_dims: int = 16):\n            super().__init__()\n\n            self.in_proj = nn.Linear(in_dims, hidden_dims)\n            self.out_proj = nn.Linear(hidden_dims, out_dims)\n\n        def __call__(self, x):\n            x = self.in_proj(x)\n            x = mx.maximum(x, 0)\n            return self.out_proj(x)\n\n    model = MyMLP(2, 1)\n\n    # All the model parameters are created but since MLX is lazy by\n    # default, they are not evaluated yet. Calling `mx.eval` actually\n    # allocates memory and initializes the parameters.\n    mx.eval(model.parameters())\n\n    # Setting a parameter to a new value is as simply as accessing that\n    # parameter and assigning a new array to it.\n    model.in_proj.weight = model.in_proj.weight * 2\n    mx.eval(model.parameters())"}
{"text": "Q: What is Linear in mlx?\nA: mlx.nn.layers.transformer.Linear Applies an affine transformation to the input.\n\nConcretely:\n\n.. math::\n\n    y = W^\\top x + b\n\nwhere :math:`W` has shape ``[output_dims, input_dims]``.\n\nArgs:\n    input_dims (int): The dimensionality of the input features\n    output_dims (int): The dimensionality of the output features\n    bias (bool, optional): If set to ``False`` then the layer will\n      not use a bias. Default ``True``."}
{"text": "Q: What is elu in mlx?\nA: mlx.nn.layers.activations.elu Applies the Exponential Linear Unit.\n\nSimply ``mx.where(x > 0, x, alpha * (mx.exp(x) - 1))``."}
{"text": "Q: What is sigmoid in mlx?\nA: mlx.nn.layers.activations.sigmoid Applies the element-wise function:\n\n.. math::\n    \\text{Sigmoid}(x) = \\sigma(x) = \\frac{1}{1 + \\exp(-x)}"}
{"text": "Q: What is Module in mlx?\nA: mlx.nn.layers.activations.Module Base class for building neural networks with MLX.\n\nAll the layers provided in :mod:`mlx.nn.layers` subclass this class and\nyour models should do the same.\n\nA ``Module`` can contain other ``Module`` instances or :class:`mlx.core.array`\ninstances in arbitrary nesting of python lists or dicts. The ``Module``\nthen allows recursively extracting all the :class:`mlx.core.array` instances\nusing :meth:`mlx.nn.Module.parameters`.\n\nIn addition, the ``Module`` has the concept of trainable and non trainable\nparameters (called \"frozen\"). When using :func:`mlx.nn.value_and_grad`\nthe gradients are returned only with respect to the trainable parameters.\nAll arrays in a module are trainable unless they are added in the \"frozen\"\nset by calling :meth:`freeze`.\n\n.. code-block:: python\n\n    import mlx.core as mx\n    import mlx.nn as nn\n\n    class MyMLP(nn.Module):\n        def __init__(self, in_dims: int, out_dims: int, hidden_dims: int = 16):\n            super().__init__()\n\n            self.in_proj = nn.Linear(in_dims, hidden_dims)\n            self.out_proj = nn.Linear(hidden_dims, out_dims)\n\n        def __call__(self, x):\n            x = self.in_proj(x)\n            x = mx.maximum(x, 0)\n            return self.out_proj(x)\n\n    model = MyMLP(2, 1)\n\n    # All the model parameters are created but since MLX is lazy by\n    # default, they are not evaluated yet. Calling `mx.eval` actually\n    # allocates memory and initializes the parameters.\n    mx.eval(model.parameters())\n\n    # Setting a parameter to a new value is as simply as accessing that\n    # parameter and assigning a new array to it.\n    model.in_proj.weight = model.in_proj.weight * 2\n    mx.eval(model.parameters())"}
{"text": "Q: What is PReLU in mlx?\nA: mlx.nn.layers.PReLU Base class for building neural networks with MLX.\n\nAll the layers provided in :mod:`mlx.nn.layers` subclass this class and\nyour models should do the same.\n\nA ``Module`` can contain other ``Module`` instances or :class:`mlx.core.array`\ninstances in arbitrary nesting of python lists or dicts. The ``Module``\nthen allows recursively extracting all the :class:`mlx.core.array` instances\nusing :meth:`mlx.nn.Module.parameters`.\n\nIn addition, the ``Module`` has the concept of trainable and non trainable\nparameters (called \"frozen\"). When using :func:`mlx.nn.value_and_grad`\nthe gradients are returned only with respect to the trainable parameters.\nAll arrays in a module are trainable unless they are added in the \"frozen\"\nset by calling :meth:`freeze`.\n\n.. code-block:: python\n\n    import mlx.core as mx\n    import mlx.nn as nn\n\n    class MyMLP(nn.Module):\n        def __init__(self, in_dims: int, out_dims: int, hidden_dims: int = 16):\n            super().__init__()\n\n            self.in_proj = nn.Linear(in_dims, hidden_dims)\n            self.out_proj = nn.Linear(hidden_dims, out_dims)\n\n        def __call__(self, x):\n            x = self.in_proj(x)\n            x = mx.maximum(x, 0)\n            return self.out_proj(x)\n\n    model = MyMLP(2, 1)\n\n    # All the model parameters are created but since MLX is lazy by\n    # default, they are not evaluated yet. Calling `mx.eval` actually\n    # allocates memory and initializes the parameters.\n    mx.eval(model.parameters())\n\n    # Setting a parameter to a new value is as simply as accessing that\n    # parameter and assigning a new array to it.\n    model.in_proj.weight = model.in_proj.weight * 2\n    mx.eval(model.parameters())"}
{"text": "Q: What is elu in mlx?\nA: mlx.nn.elu Applies the Exponential Linear Unit.\n\nSimply ``mx.where(x > 0, x, alpha * (mx.exp(x) - 1))``."}
{"text": "Q: What is Step in mlx?\nA: mlx.nn.layers.Step Applies the Step Activation Function.\n\nThis function implements a binary step activation, where the output is set\nto 1 if the input is greater than a specified threshold, and 0 otherwise.\n\n.. math::\n    \\text{step}(x) = \\begin{cases}\n    0 & \\text{if } x < \\text{threshold} \\\\\n    1 & \\text{if } x \\geq \\text{threshold}\n    \\end{cases}\n\nArgs:\n    threshold: The value to threshold at."}
{"text": "Q: What is TransformerEncoderLayer in mlx?\nA: mlx.nn.TransformerEncoderLayer Base class for building neural networks with MLX.\n\nAll the layers provided in :mod:`mlx.nn.layers` subclass this class and\nyour models should do the same.\n\nA ``Module`` can contain other ``Module`` instances or :class:`mlx.core.array`\ninstances in arbitrary nesting of python lists or dicts. The ``Module``\nthen allows recursively extracting all the :class:`mlx.core.array` instances\nusing :meth:`mlx.nn.Module.parameters`.\n\nIn addition, the ``Module`` has the concept of trainable and non trainable\nparameters (called \"frozen\"). When using :func:`mlx.nn.value_and_grad`\nthe gradients are returned only with respect to the trainable parameters.\nAll arrays in a module are trainable unless they are added in the \"frozen\"\nset by calling :meth:`freeze`.\n\n.. code-block:: python\n\n    import mlx.core as mx\n    import mlx.nn as nn\n\n    class MyMLP(nn.Module):\n        def __init__(self, in_dims: int, out_dims: int, hidden_dims: int = 16):\n            super().__init__()\n\n            self.in_proj = nn.Linear(in_dims, hidden_dims)\n            self.out_proj = nn.Linear(hidden_dims, out_dims)\n\n        def __call__(self, x):\n            x = self.in_proj(x)\n            x = mx.maximum(x, 0)\n            return self.out_proj(x)\n\n    model = MyMLP(2, 1)\n\n    # All the model parameters are created but since MLX is lazy by\n    # default, they are not evaluated yet. Calling `mx.eval` actually\n    # allocates memory and initializes the parameters.\n    mx.eval(model.parameters())\n\n    # Setting a parameter to a new value is as simply as accessing that\n    # parameter and assigning a new array to it.\n    model.in_proj.weight = model.in_proj.weight * 2\n    mx.eval(model.parameters())"}
{"text": "Q: What is selu in mlx?\nA: mlx.nn.layers.activations.selu Applies the Scaled Exponential Linear Unit.\n\n.. math::\n    \\text{selu}(x) = \\begin{cases}\n    \\lambda x & \\text{if } x > 0 \\\\\n    \\lambda \\alpha (\\exp(x) - 1) & \\text{if } x \\leq 0\n    \\end{cases}\n\nwhere :math:`\\lambda = 1.0507` and :math:`\\alpha = 1.67326`.\n\nSee also :func:`elu`."}
{"text": "Q: What is RMSNorm in mlx?\nA: mlx.nn.layers.RMSNorm Applies Root Mean Square normalization [1] to the inputs.\n\nComputes\n\n..  math::\n\n    y = \\frac{x}{\\sqrt{E[x^2] + \\epsilon}} \\gamma\n\nwhere :math:`\\gamma` is a learned per feature dimension parameter initialized at\n1.\n\n[1]: https://arxiv.org/abs/1910.07467\n\nArgs:\n    dims (int): The feature dimension of the input to normalize over\n    eps (float): A small additive constant for numerical stability"}
{"text": "Q: What is Conv1d in mlx?\nA: mlx.nn.layers.convolution.Conv1d Applies a 1-dimensional convolution over the multi-channel input sequence.\n\nThe channels are expected to be last i.e. the input shape should be ``NLC`` where:\n    - ``N`` is the batch dimension\n    - ``L`` is the sequence length\n    - ``C`` is the number of input channels\n\nArgs:\n    in_channels (int): The number of input channels\n    out_channels (int): The number of output channels\n    kernel_size (int): The size of the convolution filters\n    stride (int, optional): The stride when applying the filter.\n        Default: 1.\n    padding (int, optional): How many positions to 0-pad the input with.\n        Default: 0.\n    bias (bool, optional): If ``True`` add a learnable bias to the output.\n        Default: ``True``"}
{"text": "Q: What is relu6 in mlx?\nA: mlx.nn.relu6 Applies the Rectified Linear Unit 6.\n\nApplies :math:`\\min(\\max(x, 0), 6)` element wise."}
{"text": "Q: What is Conv2d in mlx?\nA: mlx.nn.Conv2d Applies a 2-dimensional convolution over the multi-channel input image.\n\nThe channels are expected to be last i.e. the input shape should be ``NHWC`` where:\n    - ``N`` is the batch dimension\n    - ``H`` is the input image height\n    - ``W`` is the input image width\n    - ``C`` is the number of input channels\n\nArgs:\n    in_channels (int): The number of input channels.\n    out_channels (int): The number of output channels.\n    kernel_size (int or tuple): The size of the convolution filters.\n    stride (int or tuple, optional): The size of the stride when\n        applying the filter. Default: 1.\n    padding (int or tuple, optional): How many positions to 0-pad\n        the input with. Default: 0.\n    bias (bool, optional): If ``True`` add a learnable bias to the\n        output. Default: ``True``"}
{"text": "Q: What is tree_flatten in mlx?\nA: mlx.utils.tree_flatten Flattens a python tree to a list of key, value tuples.\n\nThe keys are using the dot notation to define trees of arbitrary depth and\ncomplexity.\n\n.. code-block:: python\n\n    from mlx.utils import tree_flatten\n\n    print(tree_flatten([[[0]]]))\n    # [(\"0.0.0\", 0)]\n\n    print(tree_flatten([[[0]]], \".hello\"))\n    # [(\"hello.0.0.0\", 0)]\n\n.. note::\n   Dictionaries should have keys that are valid python identifiers.\n\nArgs:\n    tree (Any): The python tree to be flattened.\n    prefix (str): A prefix to use for the keys. The first character is\n        always discarded.\n    is_leaf (Callable): An optional callable that returns True if the\n        passed object is considered a leaf or False otherwise.\n\nReturns:\n    List[Tuple[str, Any]]: The flat representation of the python tree."}
{"text": "Q: What is ReLU in mlx?\nA: mlx.nn.layers.activations.ReLU Applies the Rectified Linear Unit.\n\nSimply ``mx.maximum(x, 0)``."}
{"text": "Q: What is CMakeExtension in mlx?\nA: mlx.extension.CMakeExtension Describes a single extension module.\n\nThis means that all source files will be compiled into a single binary file\n``<module path>.<suffix>`` (with ``<module path>`` derived from ``name`` and\n``<suffix>`` defined by one of the values in\n``importlib.machinery.EXTENSION_SUFFIXES``).\n\nIn the case ``.pyx`` files are passed as ``sources and`` ``Cython`` is **not**\ninstalled in the build environment, ``setuptools`` may also try to look for the\nequivalent ``.cpp`` or ``.c`` files.\n\n:arg str name:\n  the full name of the extension, including any packages -- ie.\n  *not* a filename or pathname, but Python dotted name\n\n:arg list[str] sources:\n  list of source filenames, relative to the distribution root\n  (where the setup script lives), in Unix form (slash-separated)\n  for portability.  Source files may be C, C++, SWIG (.i),\n  platform-specific resource files, or whatever else is recognized\n  by the \"build_ext\" command as source for a Python extension.\n\n:keyword list[str] include_dirs:\n  list of directories to search for C/C++ header files (in Unix\n  form for portability)\n\n:keyword list[tuple[str, str|None]] define_macros:\n  list of macros to define; each macro is defined using a 2-tuple:\n  the first item corresponding to the name of the macro and the second\n  item either a string with its value or None to\n  define it without a particular value (equivalent of \"#define\n  FOO\" in source or -DFOO on Unix C compiler command line)\n\n:keyword list[str] undef_macros:\n  list of macros to undefine explicitly\n\n:keyword list[str] library_dirs:\n  list of directories to search for C/C++ libraries at link time\n\n:keyword list[str] libraries:\n  list of library names (not filenames or paths) to link against\n\n:keyword list[str] runtime_library_dirs:\n  list of directories to search for C/C++ libraries at run time\n  (for shared extensions, this is when the extension is loaded).\n  Setting this will cause an exception during build on Windows\n  platforms.\n\n:keyword list[str] extra_objects:\n  list of extra files to link with (eg. object files not imp"}
{"text": "Q: What is gelu_approx in mlx?\nA: mlx.nn.layers.activations.gelu_approx An approximation to Gaussian Error Linear Unit.\n\nSee :func:`gelu` for the exact computation.\n\nThis function approximates ``gelu`` with a maximum absolute error :math:`<\n0.0003` in the range :math:`[-6, 6]` using the following\n\n.. math::\n\n    x = x \\sigma\\left(1.60033 x \\left(1 + 0.0433603 x^2\\right)\\right)\n\nwhere :math:`\\sigma(\\cdot)` is the logistic sigmoid."}
{"text": "Q: What is silu in mlx?\nA: mlx.nn.layers.silu Applies the Sigmoid Linear Unit. Also known as Swish.\n\nApplies :math:`x \\sigma(x)` element wise, where :math:`\\sigma(\\cdot)` is\nthe logistic sigmoid."}
{"text": "Q: What is Tanh in mlx?\nA: mlx.nn.layers.activations.Tanh Applies the hyperbolic tangent function.\n\nSimply ``mx.tanh(x)``."}
{"text": "Q: What is gelu in mlx?\nA: mlx.nn.layers.activations.gelu Applies the Gaussian Error Linear Units function.\n\n.. math::\n    \\\\textrm{GELU}(x) = x * \\Phi(x)\n\nwhere :math:`\\Phi(x)` is the Gaussian CDF.\n\nSee also :func:`gelu_approx` and :func:`gelu_fast_approx` for faster\napproximations."}
{"text": "Q: What is gelu in mlx?\nA: mlx.nn.gelu Applies the Gaussian Error Linear Units function.\n\n.. math::\n    \\\\textrm{GELU}(x) = x * \\Phi(x)\n\nwhere :math:`\\Phi(x)` is the Gaussian CDF.\n\nSee also :func:`gelu_approx` and :func:`gelu_fast_approx` for faster\napproximations."}
{"text": "Q: What is OptimizerState in mlx?\nA: mlx.optimizers.OptimizerState The optimizer state implements a recursively defined\n:class:`collections.defaultdict`, namely a missing key in an optimizer\nstate is an :class:`OptimizerState`.\n\n.. note::\n   :meth:`OptimizerState.get` in contrast to a normal dictionary also sets\n   the key to the ``default`` value if the ``key`` was not present in the\n   dictionary."}
{"text": "Q: What is RMSNorm in mlx?\nA: mlx.nn.layers.normalization.RMSNorm Applies Root Mean Square normalization [1] to the inputs.\n\nComputes\n\n..  math::\n\n    y = \\frac{x}{\\sqrt{E[x^2] + \\epsilon}} \\gamma\n\nwhere :math:`\\gamma` is a learned per feature dimension parameter initialized at\n1.\n\n[1]: https://arxiv.org/abs/1910.07467\n\nArgs:\n    dims (int): The feature dimension of the input to normalize over\n    eps (float): A small additive constant for numerical stability"}
{"text": "Q: What is softplus in mlx?\nA: mlx.nn.softplus Applies the Softplus function.\n\nApplies :math:`\\log(1 + \\exp(x))` element wise."}
{"text": "Q: What is gelu_fast_approx in mlx?\nA: mlx.nn.layers.activations.gelu_fast_approx A fast approximation to Gaussian Error Linear Unit.\n\nSee :func:`gelu` for the exact computation.\n\nThis function approximates ``gelu`` with a maximum absolute error :math:`<\n0.015` in the range :math:`[-6, 6]` using the following\n\n.. math::\n\n    x = x \\sigma\\left(1.773 x\\right)\n\nwhere :math:`\\sigma(\\cdot)` is the logistic sigmoid."}
{"text": "Q: What is celu in mlx?\nA: mlx.nn.layers.activations.celu Applies the Continuously Differentiable Exponential Linear Unit.\n\nApplies :math:`\\max(0, x) + \\min(0, \\alpha * (\\exp(x / \\alpha) - 1))`\nelement wise."}
{"text": "Q: What is Step in mlx?\nA: mlx.nn.layers.activations.Step Applies the Step Activation Function.\n\nThis function implements a binary step activation, where the output is set\nto 1 if the input is greater than a specified threshold, and 0 otherwise.\n\n.. math::\n    \\text{step}(x) = \\begin{cases}\n    0 & \\text{if } x < \\text{threshold} \\\\\n    1 & \\text{if } x \\geq \\text{threshold}\n    \\end{cases}\n\nArgs:\n    threshold: The value to threshold at."}
{"text": "Q: What is softplus in mlx?\nA: mlx.nn.layers.softplus Applies the Softplus function.\n\nApplies :math:`\\log(1 + \\exp(x))` element wise."}
{"text": "Q: What is Any in mlx?\nA: mlx.nn.layers.transformer.Any Special type indicating an unconstrained type.\n\n- Any is compatible with every type.\n- Any assumed to have all methods.\n- All values assumed to be instances of Any.\n\nNote that all the above statements are true from the point of view of\nstatic type checkers. At runtime, Any should not be used with instance\nchecks."}
{"text": "Q: What is Linear in mlx?\nA: mlx.nn.layers.Linear Applies an affine transformation to the input.\n\nConcretely:\n\n.. math::\n\n    y = W^\\top x + b\n\nwhere :math:`W` has shape ``[output_dims, input_dims]``.\n\nArgs:\n    input_dims (int): The dimensionality of the input features\n    output_dims (int): The dimensionality of the output features\n    bias (bool, optional): If set to ``False`` then the layer will\n      not use a bias. Default ``True``."}
{"text": "Q: What is Optimizer in mlx?\nA: mlx.optimizers.Optimizer The base class for all optimizers. It allows us to implement an\noptimizer on a per-parameter basis and apply it to a parameter tree.\n\nAttributes:\n    state (OptimizerState): It holds the optimizer's state dictionary."}
{"text": "Q: What is Extension in mlx?\nA: mlx.extension.Extension Describes a single extension module.\n\nThis means that all source files will be compiled into a single binary file\n``<module path>.<suffix>`` (with ``<module path>`` derived from ``name`` and\n``<suffix>`` defined by one of the values in\n``importlib.machinery.EXTENSION_SUFFIXES``).\n\nIn the case ``.pyx`` files are passed as ``sources and`` ``Cython`` is **not**\ninstalled in the build environment, ``setuptools`` may also try to look for the\nequivalent ``.cpp`` or ``.c`` files.\n\n:arg str name:\n  the full name of the extension, including any packages -- ie.\n  *not* a filename or pathname, but Python dotted name\n\n:arg list[str] sources:\n  list of source filenames, relative to the distribution root\n  (where the setup script lives), in Unix form (slash-separated)\n  for portability.  Source files may be C, C++, SWIG (.i),\n  platform-specific resource files, or whatever else is recognized\n  by the \"build_ext\" command as source for a Python extension.\n\n:keyword list[str] include_dirs:\n  list of directories to search for C/C++ header files (in Unix\n  form for portability)\n\n:keyword list[tuple[str, str|None]] define_macros:\n  list of macros to define; each macro is defined using a 2-tuple:\n  the first item corresponding to the name of the macro and the second\n  item either a string with its value or None to\n  define it without a particular value (equivalent of \"#define\n  FOO\" in source or -DFOO on Unix C compiler command line)\n\n:keyword list[str] undef_macros:\n  list of macros to undefine explicitly\n\n:keyword list[str] library_dirs:\n  list of directories to search for C/C++ libraries at link time\n\n:keyword list[str] libraries:\n  list of library names (not filenames or paths) to link against\n\n:keyword list[str] runtime_library_dirs:\n  list of directories to search for C/C++ libraries at run time\n  (for shared extensions, this is when the extension is loaded).\n  Setting this will cause an exception during build on Windows\n  platforms.\n\n:keyword list[str] extra_objects:\n  list of extra files to link with (eg. object files not imp"}
{"text": "Q: What is Dtype in mlx?\nA: mlx.core.Dtype An object to hold the type of a :class:`array`.\n\nSee the :ref:`list of types <data_types>` for more details\non available data types."}
