# Chapter 2 - Menny, the Sassy Transformer
![menny-the-sassy-transformer.png](images%2Fmenny-the-sassy-transformer.png)
Welcome to the world of MLX, where the ordinary becomes extraordinary. In our previous journey with the first book, we acquainted ourselves with _Tenny, the Transformer_, and his intriguing Sentiment Analyst companion. This time, brace yourself for an adventure with Menny, the Sassy Transformer.

No, we're not embarking on a daunting task of coding a Transformer from the ground up. We'll leverage the robust implementations provided by PyTorch and MLX. But make no mistake, Menny is not your average Transformer. Unlike Tenny, Menny is in her early stages of learning, not yet a fully-fledged LLM. This journey with Menny is designed to illustrate the entire spectrum of an AI project, even if the code may seem challenging at first glance.

We're not just coding here; we're exploring the universe of AI, uncovering the common pitfalls and learning how to navigate them. It's about grasping the big picture, understanding the process from start to finish. So, let's not get bogged down in details for now. They can always be revisited later.

## Menny, the Sassy Transformer - What Is She?

Menny is sassy, indeed. As a Transformer, she's adept at processing and generating sequences of words. However, her current skillset is quite unique. Menny enjoys playing with words, often responding with the exact opposite of what you say, but maintaining the same character length and structure. She transforms consonants to vowels, upper case to lower case, and so on. It's a playful, yet simple exercise in character transformation.

The dataset we're using is small and synthetic, perfect for our purpose. Here's a glimpse of the code that generates it:

```python
import random
import string

CONSONANTS = 'bcdfghjklmnpqrstvwxyzBCDFGHJKLMNPQRSTVWXYZ'
VOWELS = 'aeiouAEIOU'
LETTERS = string.ascii_letters

def generate_dataset(num_samples):
    query_list = [''.join(random.choices(LETTERS, k=random.randint(1, 5))) for _ in range(num_samples)]
    label_list = [''.join([random.choice(VOWELS.upper() if char.islower() else VOWELS.lower()) if char.isalpha() else random.choice(CONSONANTS) for char in query]) for query in query_list]

    return query_list, label_list

# Generate 100 data points
query_list, label_list = generate_dataset(100)

print(query_list[:10])  # Display the first 10 samples
print(label_list[:10])  # Display the first 10 labels
```

Understanding this code is a fundamental step. If it seems unclear, a quick refresher in Python might be helpful.

### Step 1 in Any AI Project - Define Your Problem and Generate a Dataset

Here's what I got running the above code:

```python
['U', 'kKA', 'PPXm', 'x', 'tHAxe', 'SzAZR', 'sLTtG', 'RkhiY', 'iwS', 'KzZhB']
['u', 'Uoi', 'iaoA', 'I', 'UeuAE', 'iUeee', 'OeiOe', 'iAEIa', 'UOa', 'eOoIu']
```

The dataset generated by the above code is a perfect starting point. It's straightforward and easily scalable.

In every AI project, the initial step is crucial: defining your problem and creating a dataset that accurately represents it. For our purpose, we need a dataset that allows a Transformer to understand and generate sequences of words.

Choosing a Transformer for this task might seem like an overkill compared to a simpler regression model, but remember, our aim is to steer clear of monotony. We're here to explore, to challenge ourselves, and to delve into the fascinating world of AI.

Every AI project starts with a clear problem definition and a representative dataset. It sounds simple, but it's the foundation upon which we build our AI models. Let's embark on this journey with Menny and see where it takes us.

While some may argue that a simpler model could handle this task, it's crucial to understand that we're dealing with sequences of words, both in understanding and generation. This is where a Transformer model truly shines. RNNs and LSTMs, though capable of sequence processing, fall short of the robustness offered by Transformers. Simpler models tend to struggle with longer sequences, often failing to retain the entire context. We've explored these avenues before and learned from those experiences. There's no need to tread the same path again.

If your interaction with Menny involves single characters, the complexity of sequence-aware architectures isn't necessary. In such cases, a straightforward regression model is more than adequate. However, when it comes to sequences of words, the scenario changes significantly. This situation aligns more closely with classification rather than regression. But let's not dive too deep into these specifics just yet. We'll explore these nuances in due time. For now, keeping our focus on the broader picture is key. We'll gradually unravel the finer details as we progress.

Embrace the tools at your disposal. If your framework already offers an efficient solution, like MLX does with its Transformer implementation, it's wise to utilize it. Avoid the pitfalls of reinventing the wheel or compromising on effectiveness. Efficiency and quality should be your guiding principles.

To bridge the gap between familiar and new territories, I've crafted Menny in both PyTorch and MLX. Consider the PyTorch version your final foray in this domain for this book. Moving forward, our journey will be exclusively through the landscapes of MLX. It's time to fully immerse ourselves in what MLX has to offer and discover its capabilities through practical examples. Let's embark on this new chapter with MLX leading the way.

```python
import torch.nn as nn

class Menny(nn.Module):
    def __init__(self):
        super(Menny, self).__init__()
        num_heads = 4
        self.transformer = nn.Transformer(input_size, num_heads)
        self.fc = nn.Linear(input_size, input_size)

    def forward(self, x):
        x = self.transformer(x, x)
        x = x.contiguous().view(x.size(0), -1, x.size(-1))  # Use the size of the first and last dimensions of x
        x = self.fc(x)
        return x

```

Here's MLX Menny:

```python
import mlx.nn as nn

class Menny(nn.Module):
    def __init__(self):
        super(Menny, self).__init__()
        num_heads = 4
        self.transformer = nn.Transformer(input_size, num_heads)
        self.fc = nn.Linear(input_size, input_size)

    def __call__(self, x):
        x = self.transformer(x, x, None, None, None)
        x = x.reshape(x.shape[0], -1, x.shape[-1])
        x = self.fc(x)
        return x.reshape(-1, input_size)  # Reshaping to [batch_size * QUERY_LENGTH, num_classes]
```

Indeed, the distinction is subtle. The primary variation lies in the `__call__` method used in MLX. This method serves as syntactic sugar, enabling direct invocation of the model without the explicit need to call the `forward` method. It's a nuanced difference, yet it was deliberately chosen by the MLX team. This design choice streamlines the coding process, reflecting MLX's commitment to user-friendly and intuitive programming practices.

Both PyTorch and MLX adopt the principles laid out in "Attention is All You Need," faithfully implementing the Transformer architecture. The core architecture remains consistent across both frameworks. The distinctions lie primarily in the syntax and some subtle nuances. These differences, while minor, reflect each framework's unique approach to implementing the Transformer model. With MLX Menny, we need to set some of the arguments to `None` to ensure the model's correct functioning.

```python
x = self.transformer(x, x, None, None, None) # src, tgt, src_mask, tgt_mask, memory_mask, respectively
```

No need to get bogged down in the finer details just yet. Let's refocus on our dataset. Normally, datasets are extensive and stored externally. But for our purpose, we're keeping it straightforward by generating our dataset on the fly. This method suits our synthetic dataset well, though it's not the most scalable for larger datasets. In such cases, external storage and loading are more efficient, a technique we'll delve into later.

Our dataset comprises two essential lists: `query_list` and `label_list`. The `query_list` holds our input sequences, while the `label_list` contains the corresponding transformed outputs. For instance, if the first sample in our dataset is `U`, its transformed counterpart in `label_list` is `u`. Similarly, `kKA` from `query_list` corresponds to `Uoi` in `label_list`. Both the input and label sequences maintain the same length, with character transformations following a consistent pattern. The key difference is that the labels are the inverses of the inputs.

In essence, the query list provides the inputs, and the label list offers the expected outputs for Menny to learn. The objective is to train Menny to accurately transform the input queries into their respective label sequences. These labels act as a benchmark for Menny's performance. When Menny's outputs align with these labels, she's on the right path. If there's a mismatch, it indicates a need for adjustment, and this is where loss functions come into play, guiding Menny's learning process.

```python
MODEL_WEIGHTS_FILE = 'menny_model_weights.npz'

NUM_SAMPLES = 500
NUM_EPOCHS = 100
QUERY_LENGTH = 5
BATCH_SIZE = 64
LEARNING_RATE = 0.001
MODEL_NAME = 'Menny'
```

For our training, we'll generate 500 samples and set the training duration to 100 epochs. We'll also fix the query length at 5 characters. The batch size is configured at 64, with a learning rate of 0.001. These specific values have been selected to facilitate a seamless and efficient training experience. However, I encourage you to play around with these parameters. Adjusting the batch size, learning rate, or even the number of samples and epochs can provide valuable insights. Experimenting in this manner is an excellent way to deepen your understanding of how various aspects of the training process influence the model's learning and performance.

Remarkably, even a short training period of 10 epochs can yield high accuracy. However, extending the training to 100 epochs significantly refines Menny's skills, often boosting her accuracy to near 95%. But beware, training beyond necessity can lead to overfitting. Overfitting is akin to Menny cramming for a test; she might answer all the questions correctly, but she's not truly understanding or learning. She's just memorizing. This becomes apparent when faced with new, unseen questions, where she might fail. Overfitting is a common challenge in AI and understanding how to avoid it is crucial.

As for our model, it's aptly named `Menny`, and the file containing its trained parameters is `menny_model_weights.npz`. This file, using the efficient NPZ format native to NumPy and adopted by MLX, stores Menny's learned parameters - a combination of weights and biases. These parameters can be saved and reloaded using `save_weights` and `load_weights` methods, as shown:

```python
model.save_weights('menny_model_weights.npz')
model.load_weights('menny_model_weights.npz')
```

But what exactly are these "weights"? In machine learning, the terms "weights" and "parameters" are often used interchangeably to describe the learned aspects of a model. These parameters include both weights and biases, forming the model's internal representation of the data. They are what the model adjusts through training to better understand and predict the data.

```text
y_hat = Wx + b
```

Here's a simplified breakdown:

- `y_hat` is Menny's predicted output.
- `W` represents the weight matrix.
- `x` is the input.
- `b` denotes the bias.

The weight matrix `W` is what the model learns from the data. It's crucial for the model's predictions. The bias `b`, often a constant, adjusts the output along with the weights during training through a process known as gradient descent. Both weights and biases are stored in the model weights file, `menny_model_weights.npz`, and are integral to Menny's learning and performance. Without the weights, Menny would be clueless, unable to make sense of the data. The weights are what enable Menny to learn and make predictions.

### Software 1.0 vs. Software 2.0: Embracing the AI Paradigm

In the realm of programming, there's a fundamental shift underway, moving from traditional coding methods (Software 1.0) to a more data-driven approach (Software 2.0), particularly in tasks that seem deceptively simple. You might question the necessity of AI for straightforward tasks, like transforming characters. Why not just write a function for that? This query leads us to the crux of the difference between Software 1.0 and Software 2.0.

In the traditional Software 1.0 model, a programmer explicitly codes every rule and logic. For instance, if we need to perform a basic arithmetic operation like `y = 10x * 3`, the programmer writes the exact instructions to achieve this. The programmer has complete control, and the onus of the code’s behavior and correctness rests solely on their shoulders. This method is highly structured, with the programmer defining and understanding every aspect of the code.

However, in the AI-driven world of Software 2.0, this paradigm shifts dramatically. Here, the programmer's role is not to write the explicit instructions but to create a framework where the model learns these instructions from the data. For instance, when training Menny, she doesn’t inherently understand concepts like words, consonants, vowels, or case sensitivity. Instead, she learns to respond to queries based on the patterns and relationships found in the training data. The programmer provides the data and sets up the learning environment, but it’s the AI model that discerns the underlying rules and relationships.

This shift to Software 2.0 represents a fundamental change in how we approach problem-solving in programming. Unlike Software 1.0, where the code is static and the programmer's expertise dictates the outcome, Software 2.0 is dynamic, driven by data, and less reliant on the programmer's input for the code's behavior. In Software 2.0, the quality and nature of the data become paramount. The model’s behavior, its learning, and its ability to generalize or adapt to new scenarios are shaped by the dataset it is trained on.

Software 2.0, therefore, represents a more flexible and adaptive approach to programming, especially suited to tasks where defining explicit rules is complex or impractical. It's a paradigm where the model, trained on data, uncovers the algorithm or pattern, marking a significant shift from the traditional programmer-centric approach of Software 1.0. This approach is not just about automation; it's about enabling solutions that adapt, learn, and evolve, which is at the heart of AI and machine learning.

Yes, that's what AI is all about.

### Numbers, Numbers, Numbers - The Importance of Data

Especially tn the world of AI, all data should be converted into numbers. This process is known as vectorization. It's a crucial step in the AI process, enabling the model to understand and learn from the data. In our case, we're dealing with characters, so we'll need to convert them into numbers. This conversion is known as character encoding. There are several ways to encode characters, but for our purpose, we'll use the ASCII encoding scheme. This scheme assigns a unique number to each character, enabling us to convert characters into numbers and vice versa.

In more complex scenarios, where we're dealing with words, sentences, or paragraphs, we'll need to use a more sophisticated encoding scheme. We'll explore these methods in due time. For now, let's focus on our dataset and the ASCII encoding scheme.

```python
# Convert characters to integers
CHAR_TO_INT = {char: i for i, char in enumerate(string.ascii_letters)}
INT_TO_CHAR = {i: char for char, i in CHAR_TO_INT.items()}

# Global lookup tables
ENCODE_LOOKUP = {char: i for i, char in enumerate(string.ascii_letters)}
DECODE_LOOKUP = {i: char for i, char in enumerate(string.ascii_letters)}


def encode_string(s):
    return [ENCODE_LOOKUP[char] for char in s]


def decode_array(arr):
    return [DECODE_LOOKUP.get(i, '?') for i in arr]
```

The above code creates two lookup tables, `CHAR_TO_INT` and `INT_TO_CHAR`, which map characters to integers and vice versa. The `encode_string` function converts a string into an array of integers, while the `decode_array` function converts an array of integers back into a string. These functions are crucial for our purpose, enabling us to convert characters into numbers and vice versa.

Don't expect just feeding words into Menny would work. She needs numbers, not words. 

Furthermore, she needs tensors or arrays in MLX terminology.

```python
def prepare_arrays(encoded_strings, max_length, input_size, is_label=False):
    # Initialize a 3D numpy array with zeros
    arr = np.zeros((len(encoded_strings), max_length, input_size), dtype=np.float32 if not is_label else np.int32)

    # Loop through each encoded string and set the corresponding positions to 1
    for i, encoded_string in enumerate(encoded_strings):
        for j, char_idx in enumerate(encoded_string):
            if j < max_length:
                arr[i, j, char_idx] = 1

    return mx.array(arr)
```

The above code creates a 3D NumPy array, `arr`, with zeros. This array is then populated with 1s at the corresponding positions. The `encoded_strings` parameter is a list of encoded strings, and the `max_length` parameter is the maximum length of the encoded strings. The `input_size` parameter is the size of the vocabulary, which is 52 in our case: 26 lowercase letters and 26 uppercase letters. The `is_label` parameter is a Boolean flag indicating whether the array is a label or not. The `prepare_arrays` function returns  `mx.array(arr)`, which is a 3D mx array.

Many errors in programming, particularly those that are difficult to trace, stem from misunderstandings about data, its types, and shapes. Incompatibilities in data types and discrepancies in data shapes are often the culprits behind these troublesome errors. A practical approach to mitigate these issues is through visualization. By visually representing arrays and tensors, you gain a clearer understanding of the data structure, which can preempt many potential errors. Additionally, incorporating print statements in your code to display data and their dimensions can be an invaluable debugging tool. This straightforward technique helps in pinpointing issues more efficiently.

When seeking assistance from your GPT colleagues, it's beneficial to provide details about the data shapes and types. This information is crucial as it allows them to grasp the context better and offer more accurate and helpful advice. Sharing these specifics streamlines the problem-solving process, making it easier for your GPT peers to assist you effectively.

The essential point to remember is the importance of converting your data into tensors or arrays. Since Menny operates on numerical data rather than words, it's crucial to vectorize your data into these structured forms. This process of vectorization, transforming data into tensors or arrays, is fundamental for Menny to process and learn from the data effectively.

## Step 2 in Any AI Project - Designing Your Model

Designing an appropriate model is a critical step in any AI project. In our case, we've chosen a Transformer model for its efficacy in handling sequences. Here's a glimpse of what the Menny model might look like:

```python
class Menny(nn.Module):
    def __init__(self):
        super(Menny, self).__init__()
        num_heads = 4
        self.transformer = nn.Transformer(input_size, num_heads)
        self.fc = nn.Linear(input_size, input_size)

    def __call__(self, x):
        x = self.transformer(x, x, None, None, None)
        x = x.reshape(x.shape[0], -1, x.shape[-1])
        x = self.fc(x)
        return x.reshape(-1, input_size)  # Reshaping to [batch_size * QUERY_LENGTH, num_classes]
```

In this model, the `nn.Transformer` class plays a crucial role. It inherently includes an activation function, typically a softmax layer, which is essential for making predictions. This softmax layer at the end of the Transformer architecture is why we don't explicitly define an additional activation function in our model.

The model's output is reshaped to `[batch_size * QUERY_LENGTH, num_classes]`, aligning it with the required format for the next steps in processing. Here, `batch_size` represents the number of samples in each batch, `QUERY_LENGTH` is the length of each input sequence, and `num_classes` denotes the total number of possible classes. For our scenario, with both lowercase and uppercase letters, `num_classes` is 52.

This reshaping is vital to ensure compatibility with the labels, which are structured as `[batch_size, QUERY_LENGTH]`. When considering a dataset of 500 samples and training over 100 epochs with batching, we're looking at about 5000 iterations. Batching here is not just a preference but a necessity for efficient training. Without it, even with powerful GPUs, the training process would be considerably slower and less efficient. This efficiency is especially crucial in large-scale models or datasets, where the computational load can be significant. In a Transformer model, the choice of num_heads is closely related to the `input_size` (or dimensionality of the model), and it's not just a matter of model capacity or complexity. Specifically, the `input_size` must be divisible by num_heads for the model to function correctly. If this condition is not met, it can indeed result in an error.

The `num_heads` parameter in the Transformer model, set to 4 in this case, refers to the number of attention heads in the multi-head attention mechanism of the Transformer. Understanding the significance of this parameter requires a look into how multi-head attention works and why it's beneficial in Transformer architectures.

In summary, the design of your model should always cater to the specifics of your task, considering not just the nature of the data but also the practical aspects of training and inference. The Menny model exemplifies this, balancing the need for effective sequence processing with the practicalities of training efficiency.

### Step 3 in Any AI Project - Embracing Batching for Efficient Training

Venturing into model training without batching is technically possible, but brace yourself for a test of patience. Batching stands as an essential pillar in the training process, ensuring both efficiency and effectiveness. Without it, the training can become a sluggish affair. To illustrate, even on my M2 Mac Studio Ultra, a single epoch without batching felt like an eternity. Batching transforms this scenario, accelerating the training and enhancing the precision of the outcomes.

The impact of batch sizes extends beyond just training speed; it plays a pivotal role in determining the model's performance. Generally, larger batch sizes can lead to more accurate results. However, they also demand more memory, creating a balancing act between accuracy and resource allocation. For our project, a batch size of 64 strikes a harmonious balance, offering a good compromise between precision and memory usage. Yet, this is not a one-size-fits-all solution. Adjusting the batch size based on your specific requirements and constraints can be insightful. Experimenting with various batch sizes offers a practical learning experience, deepening your understanding of their effects on both the training dynamics and the final model performance. This experimentation is crucial, especially in scenarios where resource constraints are a significant consideration.

```python
def batch_iterate(batch_size, X, y):
    num_batches = len(X) // batch_size
    for i in range(num_batches):
        start_index = i * batch_size
        end_index = start_index + batch_size
        yield X[start_index:end_index], y[start_index:end_index]


def train(model, input_arrays, label_arrays):
    for epoch in range(NUM_EPOCHS):
        print(f'Starting epoch {epoch + 1}/{NUM_EPOCHS}')

        # Create batches
        for input_batch, target_batch in batch_iterate(BATCH_SIZE, input_arrays, label_arrays):
            # Compute loss and gradients
            loss, grads = loss_and_grad_fn(model, input_batch, target_batch)

            # Update the model
            optimizer.update(model, grads)
            mx.eval(model.parameters(), optimizer.state)

        if epoch % 10 == 9:
            print(f'Epoch {epoch + 1}/{NUM_EPOCHS}, Loss: {loss.item()}')

```

In the complex world of data handling for machine learning, sophisticated approaches become essential. In PyTorch, this complexity is elegantly managed by the `Dataset` and `DataLoader` classes, which streamline the data processing workflow, as demonstrated in the previous book. However, in the realm of MLX, a different strategy is proposed: `MLX Data`. This is a framework-agnostic data loading library developed by Apple's machine learning research team. Given its complexity and evolving nature, MLX Data deserves its own dedicated exploration, which I plan to undertake in a future chapter. As it's still a developing framework, it's prudent to allow it some time to mature before fully integrating it into our workflow.

In the meantime, we'll employ the `batch_iterate` function for our purposes. This function is commonly featured in many official MLX examples, often alongside `batch_iterate` or similar functions, or even `Dataset` and `DataLoader` wrappers. The presence of these functionalities hints at the nascent stage of `MLX Data`.

You can explore more about MLX Data here: [MLX Data GitHub Repository](https://github.com/ml-explore/mlx-data).

The `batch_iterate` function is relatively straightforward in its operation. It requires three key parameters: the batch size (`BATCH_SIZE`), and the input and label arrays (`input_arrays` and `label_arrays`). The function efficiently cycles through these arrays, assembling them into batches of the specified size. This method is a practical solution for managing data in our current MLX projects and serves as a reliable interim approach until MLX Data becomes more established and robust.

One essential aspect to keep in mind during model development is the necessity for the model to be cognizant of batch processing. This is reflected in the inclusion of a batch dimension in the model's architecture. The batch dimension plays a pivotal role across various stages of the model's lifecycle, including training, testing, and inference.

When working with this batch dimension, it's crucial to ensure consistency in its handling throughout the model. Whether you're in the midst of training, conducting tests, or deploying the model for inference, the batch dimension must be consistently accounted for and matched accordingly. This means that the model should expect and process input data in batches of a specified size, and this batch size needs to be uniformly maintained across all functions and stages of the model's use.

Neglecting to match the batch dimension can lead to errors or inconsistencies in the model's performance, as the model's architecture is designed to process data in these batched formats. Therefore, a thorough understanding and careful management of the batch dimension are vital for the smooth operation and optimal performance of the model.

Batching, while efficient, often brings its own set of challenges, particularly with batch dimension-related errors. If you find yourself facing an unexpected error during batching, a good first step is to scrutinize the batch dimension. Many times, these issues stem from mismatches or misalignments in the batch dimension. Addressing this early can save you considerable time and effort, preventing the need for more extensive debugging. So, always keep an eye on the batch dimension - it's often the key to quickly resolving those tricky errors that crop up with batching.

```bash
File "/Users/wankyuchoi/anaconda3/envs/mlx/lib/python3.11/site-packages/mlx/nn/layers/transformer.py", line 84, in __call__
    B, L, D = queries.shape
    ^^^^^^^
ValueError: not enough values to unpack (expected 3, got 2)
```

In cases like our example with the Transformer, discrepancies in the batch dimension can lead to perplexing errors that might seem daunting at first. These errors, often elusive, can necessitate an in-depth analysis to identify their origins. However, by paying close attention to the batch dimension from the outset, you can swiftly address and rectify these issues. Keeping a vigilant eye on the batch dimension is a crucial practice, as it often holds the key to unraveling and fixing these otherwise puzzling Transformer errors.

### Step 4 in Any AI Project - Defining Your Loss Function

Well, optimizers are rather easy. Adam is a good choice. 

```python
optimizer = optim.Adam(learning_rate=LEARNING_RATE)
```

However, the loss function is a crucial component of any AI project. It's the guiding force that steers the model's learning process, enabling it to discern the underlying patterns and relationships in the data. In our case, we're dealing with a classification task, where the model needs to learn to transform the input sequences into their respective labels. This task is well-suited to the `cross-entropy` loss function, which is designed for classification problems. This loss function is a popular choice for classification tasks, and it's the one we'll be using for Menny.

```python
def loss_fn(model, input_batch, target_batch):
    output = model(input_batch)
    target_indices = mx.argmax(target_batch, axis=2)  # Convert one-hot to indices
    return nn.losses.cross_entropy(output, target_indices.reshape(-1), reduction='none').mean()
```

And MLX is unique in their loss functions: Composable Function Transformations. See the sidebar for more details, for now.

[Composable-Function-Transformations.md](..%2F..%2Fbook%2Fsidebars%2Fcomposable-function-transformations%2FComposable-Function-Transformations.md)

We will explore more on this concept in coming chapters. We have to look at a bigger picture for now.

### Step 5 in Any AI Project - Training Your Model

Now, we're ready to train Menny. Let's take a look at the code:

```python
def train(model, input_arrays, label_arrays):
    for epoch in range(NUM_EPOCHS):
        print(f'Starting epoch {epoch + 1}/{NUM_EPOCHS}')

        # Create batches
        for input_batch, target_batch in batch_iterate(BATCH_SIZE, input_arrays, label_arrays):
            # Compute loss and gradients
            loss, grads = loss_and_grad_fn(model, input_batch, target_batch)

            # Update the model
            optimizer.update(model, grads)
            mx.eval(model.parameters(), optimizer.state)

        if epoch % 10 == 9:
            print(f'Epoch {epoch + 1}/{NUM_EPOCHS}, Loss: {loss.item()}')
```

In this step, we're focusing on the training process of Menny over `NUM_EPOCHS` epochs. During each epoch, the model processes batches, each containing `BATCH_SIZE` samples. The `train` function, efficiently designed, sequentially processes these batches. For each batch, it computes the loss and gradients, which are essential for the model's learning. Following this, the `optimizer` steps in to adjust the model's parameters in response to these gradients. This process of loss calculation, gradient computation, and parameter adjustment is repeated across all epochs. To monitor our progress, we log the loss at every tenth epoch. This method, combining batching and optimization, exemplifies a direct yet effective approach to training.

A key aspect of MLX is its "lazy" computation style. Rather than immediately calculating gradients, MLX defers these computations until they are explicitly needed. While this might initially seem counterintuitive, it's actually a beneficial approach, especially in terms of optimizing memory usage.

The following lines in our training loop are pivotal:

```python
            # Update the model
            optimizer.update(model, grads)
            mx.eval(model.parameters(), optimizer.state)
```

These lines are central to our model's ability to learn. Without them, the model would fail to evolve. In the worst cases, the training loop might continue indefinitely.

This approach is known as "_**Lazy Evaluation**_". It's a topic we'll delve into more deeply in a future chapter. For now, think of it as a methodical way to conserve computational resources. It adheres to the principle of delaying computations until absolutely necessary, thereby making efficient use of computational power. This is the crux of lazy evaluation – calculating only when required, optimizing the use of resources throughout the training process.

### Step 6 in Any AI Project - Testing Your Model

```python
def test_model(model, test_data, test_labels):
    model.eval()
    correct = 0
    total = 0

    for data, label in zip(test_data, test_labels):
        # Ensure data has batch dimension
        if len(data.shape) == 2:
            data = mx.expand_dims(data, axis=0)

        outputs = model(data)

        # Flatten the output and labels for comparison
        outputs = outputs.reshape(-1, input_size)
        label = label.reshape(-1, input_size)

        # Convert outputs to class predictions
        predicted = mx.argmax(outputs, axis=1)
        true_labels = mx.argmax(label, axis=1)

        # Update total and correct counts
        total += true_labels.size
        correct += (predicted == true_labels).sum().item()

    accuracy = 100 * correct / total
    return accuracy
```

As we proceed, our focus shifts to assessing the model's effectiveness. The `test_model` function plays a pivotal role in this phase, serving to gauge the model's accuracy. It requires the model itself, alongside the test dataset and corresponding labels for evaluation. A key step in this process is the invocation of `model.eval()`. This command switches the model into evaluation mode, a necessary state for testing. This mode is crucial as it prevents any further learning or adjustments in the model's parameters during the evaluation, ensuring the integrity of the test results.

Throughout the testing phase, we keep track of the model's performance using two variables: `correct` and `total`. `correct` tallies the number of accurate predictions made by the model, while `total` counts the overall number of predictions. By comparing these values, we can calculate the model's accuracy at the end of the test.

In a more comprehensive real-world scenario, as outlined in the first book, a three-tiered data approach is typically employed: `training`, `validation`, and `testing`. The training set is used for the initial model training, the validation set for fine-tuning the hyperparameters, and the testing set for an unbiased evaluation of the final model. This method is often referred to as the "train, tune, and test" strategy. For our current project, we've simplified the process by focusing primarily on the training and testing phases.

Had we adopted the comprehensive "train, tune, and test" approach for our dataset of 500 samples, a practical division might have been allocating 300 samples for training, 100 for validation, and the remaining 100 for testing. This division is strategically designed to safeguard against overfitting and underfitting, two common pitfalls in AI model development.

Note that validation occurs _during the training phase_, while testing is conducted _after the training is complete_. The validation set is used to fine-tune the model's hyperparameters, ensuring that it's not overfitting or underfitting. 

Overfitting occurs when a model becomes excessively tailored to the training data, to the extent that it performs poorly on any new, unseen data. It's like the model has learned the training data by heart, including its noise and anomalies, but fails to generalize its learning to broader, real-world scenarios. Underfitting is the opposite issue. It happens when the model is too simplistic, lacking the complexity needed to understand and capture the essential patterns in the data. An underfitted model overlooks the nuances, resulting in a lack of accuracy even with the training data.

The use of separate training, validation, and testing datasets is a fundamental strategy to combat these issues. The training set is used to teach the model, the validation set to adjust and fine-tune the model's parameters (helping to identify and correct overfitting or underfitting), and the testing set to evaluate the model's performance on entirely new data. This tripartite approach ensures that the model not only learns effectively but also remains versatile and accurate when confronted with fresh data. Understanding and implementing these strategies is crucial in AI to develop models that are both reliable and robust. 

### Step 7 in Any AI Project - Deploying Your Model

Simulating the deployment of Menny in a real-world context involves setting up an interactive session where we can test her capabilities with our input prompts. This final step in our project allows us to actively engage with Menny, bringing the theoretical aspects of our work into a practical, tangible realm.

```python
def query_model(model):
    # Ask for user input
    input_string = input(f"Enter your prompt (max {QUERY_LENGTH} chars, q to quit): ")
    if input_string == 'q':
        return None

    # Truncate the input_string to match QUERY_LENGTH
    input_string = input_string[:QUERY_LENGTH]

    # Ensure the model is in evaluation mode
    model.eval()

    # Encode the input string
    encoded_input = encode_string(input_string)

    # Prepare the tensor
    input_arr = prepare_arrays([encoded_input], QUERY_LENGTH, input_size)

    # Use numpy to add a batch dimension and convert back to MLX array
    input_np = np.expand_dims(np.array(input_arr[0]), axis=0)
    input_arr_batched = mx.array(input_np)

    output = model(input_arr_batched)

    # Flatten the output and decode each character
    output = output.reshape(-1, input_size)  # Flatten the output
    output_indices = mx.argmax(output, axis=1)
    response = decode_array(np.asarray(output_indices).flatten())

    # Join the characters in the list into a single string
    response = ''.join(response)

    # Truncate the response to match the length of the input string
    response = response[:len(input_string)]

    return response
```

The function prompts the user for input. This input is then transformed—encoded—into a format that Menny can understand and process. Once Menny processes this input, the resulting output is decoded back into a human-readable format and presented to the user. This interactive loop continues until the user decides to exit by entering `q`.

```python
    if os.path.exists(MODEL_WEIGHTS_FILE):
        # Load previously saved weights
        model.load_weights(MODEL_WEIGHTS_FILE)
        print("Loaded model weights from saved file.")
...
   else:
...
        train(model, input_arrays, label_arrays)

        # Saving the model weights after training
        model.save_weights(MODEL_WEIGHTS_FILE)
        print(f"Saved model weights to '{MODEL_WEIGHTS_FILE}'.")
```

An important aspect of working with models like Menny, especially when dealing with sizable datasets, is the ability to save and load the model's parameters. This capability is crucial for efficient workflow management. If a saved weights file is available, we load the model's parameters from this file, allowing Menny to utilize the knowledge she has already acquired. If no such file exists, we proceed with training Menny from scratch and subsequently save these newly learned parameters. This approach not only saves time but also resources, as it prevents the need to retrain the model from the beginning every time we want to use it. It ensures that Menny's learning and improvements are retained, making it possible to pause and resume training as needed or to deploy the model later with all its learned parameters intact. This method of parameter preservation is a practical and effective way to maintain and enhance Menny's performance over time.

## The Final Step - Running Your Model

Now, it's time to bring Menny to life and see her in action. We initiate the interaction with a simple command, and Menny is ready to respond to our prompts. The session is straightforward: we provide an input (with a maximum of five characters), and Menny replies with her unique, sassy transformations.

```bash
['y', 'Zkth', 'aZel', 'LXemb', 'A', 'HfK', 'cee', 'Qc', 'tKa', 'acK']
['I', 'oUEI', 'UeEU', 'aoAEE', 'i', 'iEo', 'UOO', 'oA', 'OiE', 'AIe']
Loaded model weights from saved file.
Enter your prompt (max 5 chars, q to quit): Menny
Menny: eEIIE
Enter your prompt (max 5 chars, q to quit): menny
Menny: UUUUE
Enter your prompt (max 5 chars, q to quit): MeNnY
Menny: uIuAe
Enter your prompt (max 5 chars, q to quit): mmm
Menny: UUU
Enter your prompt (max 5 chars, q to quit): mem
Menny: UEU
Enter your prompt (max 5 chars, q to quit): MeM
Menny: uIu
Enter your prompt (max 5 chars, q to quit): 
```

The system also informs us when the model's weights are loaded from a saved file, ensuring that Menny is utilizing all her previously learned knowledge. This seamless process allows for an engaging and interactive experience with Menny, showcasing her ability to playfully transform inputs in her characteristic contrarian style. As we interact with Menny, it becomes clear that she's more than just an AI model; she's a quirky character with a penchant for wordplay.

## Getting the Bigger Picture

Throughout this chapter, you've been immersed in a hands-on journey through the AI development lifecycle. We've traversed the path from dataset creation and model design to training and deployment, illuminating the process with a practical example. Along the way, you've encountered typical challenges and pitfalls inherent in AI projects, gaining insight into how to navigate these obstacles.

This experience is more than just a tutorial; it's a foundational guide to understanding the AI workflow. The knowledge gained here is not just theoretical but eminently applicable to a wide range of projects you may undertake in the future.

The key takeaway is to always keep the broader perspective in mind. The details, while important, find their true value when viewed within the context of this bigger picture. It's this holistic understanding that prevents you from getting lost in the complexities of AI.

In this chapter, certain details were deliberately omitted to help you focus on the overarching process. As you progress in your AI journey, you'll find that while models and data types (be they images, text, audio, or video) may vary, the underlying process remains remarkably consistent.

Familiarity with this process is crucial. It's a cycle you'll encounter and iterate through repeatedly, each time with potentially different data and models but always with the same fundamental steps. Embrace this process, understand it deeply, and you'll be well-equipped to tackle a myriad of AI challenges.

![menny-the-sassy-transformer.png](images%2Fmenny-the-sassy-transformer.png)

Oh, Menny? She'll learn and grow, just as we should.